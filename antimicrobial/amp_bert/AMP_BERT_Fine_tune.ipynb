{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYhGE2zhpcgo",
        "outputId": "5ae72628-0e5a-4876-d070-92d828b11f6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.26.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.8.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2023.7.22)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 24.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8wIZMbgrdGa",
        "outputId": "f6f1e00e-62fa-49e1-df7e-a05716178fa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\LukaBursic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import re\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "from transformers import AutoTokenizer, Trainer, TrainingArguments, BertForSequenceClassification, AdamW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lb579FMwrdKU"
      },
      "outputs": [],
      "source": [
        "# define a class for the AMP data that will correctly format the sequence information\n",
        "# for fine-tuning with huggingface API\n",
        "# the input dataframe columns must be formatted the same way as the given example\n",
        "\n",
        "class amp_data():\n",
        "    def __init__(self, df, tokenizer_name='Rostlab/prot_bert_bfd', max_len=200):\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, do_lower_case=False)\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.seqs, self.labels = self.get_seqs_labels()\n",
        "\n",
        "    def get_seqs_labels(self):\n",
        "        # isolate the amino acid sequences and their respective AMP labels\n",
        "        seqs = list(df['aa_seq'])\n",
        "        labels = list(df['AMP'].astype(int))\n",
        "\n",
        "#         assert len(seqs) == len(labels)\n",
        "        return seqs, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = \" \".join(\"\".join(self.seqs[idx].split()))\n",
        "        seq_ids = self.tokenizer(seq, truncation=True, padding='max_length', max_length=self.max_len)\n",
        "\n",
        "        sample = {key: torch.tensor(val) for key, val in seq_ids.items()}\n",
        "        sample['labels'] = torch.tensor(self.labels[idx])\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBymtbXgrdOc",
        "outputId": "7b4838ec-928f-486f-ab66-96f14c8d0078"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                            aa_seq  aa_len  \\\n",
            "AP02151          YEALVTSILGKLTGLWHNDSVDFMGHICYFRRRPKIRRFKLYHEGK...      95   \n",
            "AP01951                                          FLPLVLGALSGILPKIL      17   \n",
            "AP00972                                        FLSLIPHAINAVGVHAKHF      19   \n",
            "AP01261                                           IIEKLVNTALGLLSGL      16   \n",
            "AP01298                                       GLFTLIKCAYQLIAPTVACN      20   \n",
            "AP01802                                     RPWAGNGSVHRYTVLSPRLKTQ      22   \n",
            "UniRef50_Q9UTR1                                SKENSYVEKLLYKQRFYAS      19   \n",
            "\n",
            "                   AMP  \n",
            "AP02151           True  \n",
            "AP01951           True  \n",
            "AP00972           True  \n",
            "AP01261           True  \n",
            "AP01298           True  \n",
            "AP01802           True  \n",
            "UniRef50_Q9UTR1  False  \n"
          ]
        }
      ],
      "source": [
        "# read in the train dataset\n",
        "# create an amp_data class of the dataset\n",
        "\n",
        "data_url = 'https://raw.githubusercontent.com/GIST-CSBL/AMP-BERT/main/all_veltri.csv'\n",
        "df = pd.read_csv(data_url, index_col = 0)\n",
        "df = df.sample(frac=1, random_state = 0)\n",
        "print(df.head(7))\n",
        "\n",
        "train_dataset = amp_data(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "n4jrGQT6rdRA"
      },
      "outputs": [],
      "source": [
        "# define the necessary metrics for performance evaluation\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "#     conf = confusion_matrix(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "#         'confusion matrix': conf\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "j-pQako9rdTQ"
      },
      "outputs": [],
      "source": [
        "# define the model initializing function for Trainer in huggingface\n",
        "\n",
        "def model_init():\n",
        "    return BertForSequenceClassification.from_pretrained('Rostlab/prot_bert_bfd')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.31.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (1.26.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in c:\\users\\lukabursic\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (5.9.8)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (2.1.1)\n",
            "Requirement already satisfied: huggingface-hub in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (0.23.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.8.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
            "Requirement already satisfied: requests in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\lukabursic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 24.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate -U\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'d:\\\\Faks\\\\4. godina\\\\Evolucijsko računarstvo\\\\Projekt\\\\antimicrobial\\\\amp_bert'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hk0ETN8brdVR",
        "outputId": "453f5e97-7bd8-4689-cc77-5549f60ff358"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\LukaBursic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "expected str, bytes or os.PathLike object, not NoneType",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[19], line 39\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(logging_dir), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogging_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a directory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     16\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# output_dir='./results',\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     load_best_model_at_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     37\u001b[0m )\n\u001b[1;32m---> 39\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
            "File \u001b[1;32mc:\\Users\\LukaBursic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:585\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_hf_repo()\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m--> 585\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_collator) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_collator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollate_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[0;32m    588\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `data_collator` should be a simple callable (function, class with `__call__`).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32m<frozen os>:210\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n",
            "File \u001b[1;32m<frozen ntpath>:213\u001b[0m, in \u001b[0;36msplit\u001b[1;34m(p)\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
          ]
        }
      ],
      "source": [
        "output_dir = './results'\n",
        "logging_dir = './logs'\n",
        "\n",
        "# Create directories if they do not exist\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "    print(f\"Created output directory: {output_dir}\")\n",
        "if not os.path.exists(logging_dir):\n",
        "    os.makedirs(logging_dir)\n",
        "    print(f\"Created logging directory: {logging_dir}\")\n",
        "\n",
        "# Verify the directories are created and accessible\n",
        "assert os.path.isdir(output_dir), f\"{output_dir} is not a directory\"\n",
        "assert os.path.isdir(logging_dir), f\"{logging_dir} is not a directory\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=15,\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=1,\n",
        "    warmup_steps=0,\n",
        "    weight_decay=0.1,\n",
        "    logging_dir='D:\\\\Faks\\\\4. godina\\\\Evolucijsko računarstvo\\\\Projekt\\\\antimicrobial\\\\amp_bert\\\\logs',\n",
        "    logging_steps=100,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy=\"no\",\n",
        "    save_strategy='no',\n",
        "    gradient_accumulation_steps=64,\n",
        "    fp16=False,\n",
        "    fp16_opt_level=\"O2\",\n",
        "    run_name=\"AMP-BERT\",\n",
        "    seed=0,\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model_init=model_init,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.path.isdir(\"D:\\\\Faks\\\\4. godina\\\\Evolucijsko računarstvo\\\\Projekt\\\\antimicrobial\\\\amp_bert\\\\logs\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "NVOmwxYarr4w",
        "outputId": "5978628e-96a1-4adb-ec88-5c747bb8fc41"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'trainer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# performance metrics on the training data itself\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m predictions, label_ids, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(train_dataset)\n\u001b[0;32m      4\u001b[0m metrics\n",
            "\u001b[1;31mNameError\u001b[0m: name 'trainer' is not defined"
          ]
        }
      ],
      "source": [
        "# performance metrics on the training data itself\n",
        "\n",
        "predictions, label_ids, metrics = trainer.predict(train_dataset)\n",
        "metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6JgOpbw9q4Y"
      },
      "outputs": [],
      "source": [
        "# save the model, if desired\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# trainer.save_model('/content/drive/MyDrive/Colab Notebooks/AMP-BERT/Fine-tuned_model/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqdIvYee4i_v",
        "outputId": "a8309c6f-0378-4bd4-e56c-a37eca4b72bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"Rostlab/prot_bert_bfd\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 40000,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 30,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.22.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/vocab.txt\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"Rostlab/prot_bert_bfd\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 40000,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 30,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.22.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Rostlab--prot_bert_bfd/snapshots/6c5c8a55a52ff08a664dfd584aa1773f125a0487/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"Rostlab/prot_bert_bfd\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 40000,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 30,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.22.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30\n",
            "}\n",
            "\n",
            "loading configuration file /content/drive/MyDrive/Colab Notebooks/AMP-BERT/Fine-tuned_model/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"Rostlab/prot_bert_bfd\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 40000,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 30,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.22.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/MyDrive/Colab Notebooks/AMP-BERT/Fine-tuned_model/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/AMP-BERT/Fine-tuned_model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "# predict AMP/non-AMP for a single example\n",
        "\n",
        "# IMPORTANT:\n",
        "# one must mount their Google Drive and load their own fine-tuned model before running the below cell for individual predictions\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# load appropriate tokenizer and fine-tuned model\n",
        "tokenizer = AutoTokenizer.from_pretrained('Rostlab/prot_bert_bfd', do_lower_case=False)\n",
        "model = BertForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/AMP-BERT/Fine-tuned_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3q3GujE3_csL",
        "outputId": "ca2331b1-2422-4f0f-923f-ab0220114d04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input peptide sequence: FNRGGYNFGKSVRHVVDAIGSVAGIRGILKSIR\n",
            "Class prediction: AMP\n"
          ]
        }
      ],
      "source": [
        "# predict AMP/non-AMP for a single example (default ex. is from external test data: DRAMP00126)\n",
        "\n",
        "#@markdown **Input peptide sequence (upper case only)**\n",
        "input_seq = 'FNRGGYNFGKSVRHVVDAIGSVAGIRGILKSIR' #@param {type:\"string\"}\n",
        "input_seq_spaced = ' '.join([ input_seq[i:i+1] for i in range(0, len(input_seq), 1) ])\n",
        "input_seq_spaced = re.sub(r'[UZOB]', 'X', input_seq_spaced)\n",
        "input_seq_tok = tokenizer(input_seq_spaced, return_tensors = 'pt')\n",
        "\n",
        "output = model(**input_seq_tok)\n",
        "logits = output[0]\n",
        "\n",
        "# extract AMP class probability and make binary prediction\n",
        "y_prob = torch.sigmoid(logits)[:,1].detach().numpy()\n",
        "y_pred = y_prob > 0.5\n",
        "if y_pred == True:\n",
        "  input_class = 'AMP'\n",
        "else:\n",
        "  input_class = 'non-AMP'\n",
        "\n",
        "print('Input peptide sequence: ' + input_seq)\n",
        "print('Class prediction: ' + input_class)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
