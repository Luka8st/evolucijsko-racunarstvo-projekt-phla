{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>A</th>\n",
       "      <th>R</th>\n",
       "      <th>N</th>\n",
       "      <th>D</th>\n",
       "      <th>C</th>\n",
       "      <th>Q</th>\n",
       "      <th>E</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>...</th>\n",
       "      <th>L</th>\n",
       "      <th>K</th>\n",
       "      <th>M</th>\n",
       "      <th>F</th>\n",
       "      <th>P</th>\n",
       "      <th>S</th>\n",
       "      <th>T</th>\n",
       "      <th>W</th>\n",
       "      <th>Y</th>\n",
       "      <th>V</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.215069</td>\n",
       "      <td>0.483352</td>\n",
       "      <td>0.143512</td>\n",
       "      <td>0.133588</td>\n",
       "      <td>2.217512</td>\n",
       "      <td>0.130810</td>\n",
       "      <td>0.168565</td>\n",
       "      <td>0.715721</td>\n",
       "      <td>0.064815</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428184</td>\n",
       "      <td>0.316324</td>\n",
       "      <td>0.184180</td>\n",
       "      <td>0.214149</td>\n",
       "      <td>0.257585</td>\n",
       "      <td>0.225357</td>\n",
       "      <td>0.148889</td>\n",
       "      <td>0.143170</td>\n",
       "      <td>0.103946</td>\n",
       "      <td>0.182000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.114678</td>\n",
       "      <td>0.429829</td>\n",
       "      <td>0.103611</td>\n",
       "      <td>0.101690</td>\n",
       "      <td>2.369031</td>\n",
       "      <td>0.075304</td>\n",
       "      <td>0.168214</td>\n",
       "      <td>0.340382</td>\n",
       "      <td>0.104869</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360381</td>\n",
       "      <td>0.416033</td>\n",
       "      <td>0.224845</td>\n",
       "      <td>0.165256</td>\n",
       "      <td>0.205752</td>\n",
       "      <td>0.227474</td>\n",
       "      <td>0.128598</td>\n",
       "      <td>0.096066</td>\n",
       "      <td>0.107744</td>\n",
       "      <td>0.189875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.160948</td>\n",
       "      <td>0.241044</td>\n",
       "      <td>0.097990</td>\n",
       "      <td>0.099774</td>\n",
       "      <td>0.765575</td>\n",
       "      <td>0.088194</td>\n",
       "      <td>0.103967</td>\n",
       "      <td>0.402441</td>\n",
       "      <td>0.087890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305895</td>\n",
       "      <td>0.244981</td>\n",
       "      <td>0.131540</td>\n",
       "      <td>0.123376</td>\n",
       "      <td>0.159369</td>\n",
       "      <td>0.174201</td>\n",
       "      <td>0.158651</td>\n",
       "      <td>0.104443</td>\n",
       "      <td>0.097218</td>\n",
       "      <td>0.247258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.163472</td>\n",
       "      <td>0.423496</td>\n",
       "      <td>0.105217</td>\n",
       "      <td>0.068877</td>\n",
       "      <td>1.163470</td>\n",
       "      <td>0.076649</td>\n",
       "      <td>0.104508</td>\n",
       "      <td>0.201092</td>\n",
       "      <td>0.077516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.346661</td>\n",
       "      <td>0.286027</td>\n",
       "      <td>0.160886</td>\n",
       "      <td>0.129036</td>\n",
       "      <td>0.131611</td>\n",
       "      <td>0.135475</td>\n",
       "      <td>0.142005</td>\n",
       "      <td>0.164302</td>\n",
       "      <td>0.096828</td>\n",
       "      <td>0.122887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.070393</td>\n",
       "      <td>0.165129</td>\n",
       "      <td>0.092374</td>\n",
       "      <td>0.078504</td>\n",
       "      <td>1.478498</td>\n",
       "      <td>0.075313</td>\n",
       "      <td>0.106306</td>\n",
       "      <td>0.249911</td>\n",
       "      <td>0.077612</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249818</td>\n",
       "      <td>0.137413</td>\n",
       "      <td>0.138184</td>\n",
       "      <td>0.150049</td>\n",
       "      <td>0.135389</td>\n",
       "      <td>0.133322</td>\n",
       "      <td>0.130349</td>\n",
       "      <td>0.101665</td>\n",
       "      <td>0.033102</td>\n",
       "      <td>0.104636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.132378</td>\n",
       "      <td>0.137835</td>\n",
       "      <td>0.086295</td>\n",
       "      <td>0.102439</td>\n",
       "      <td>0.446971</td>\n",
       "      <td>0.039611</td>\n",
       "      <td>0.085510</td>\n",
       "      <td>0.320189</td>\n",
       "      <td>0.061554</td>\n",
       "      <td>...</td>\n",
       "      <td>0.225524</td>\n",
       "      <td>0.181738</td>\n",
       "      <td>0.120943</td>\n",
       "      <td>0.121934</td>\n",
       "      <td>0.087696</td>\n",
       "      <td>0.076736</td>\n",
       "      <td>0.132841</td>\n",
       "      <td>0.107809</td>\n",
       "      <td>0.038939</td>\n",
       "      <td>0.098509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.160407</td>\n",
       "      <td>0.089654</td>\n",
       "      <td>0.079356</td>\n",
       "      <td>0.113461</td>\n",
       "      <td>0.372536</td>\n",
       "      <td>0.050125</td>\n",
       "      <td>0.064223</td>\n",
       "      <td>0.205711</td>\n",
       "      <td>0.082409</td>\n",
       "      <td>...</td>\n",
       "      <td>0.177923</td>\n",
       "      <td>0.236984</td>\n",
       "      <td>0.142232</td>\n",
       "      <td>0.132447</td>\n",
       "      <td>0.062499</td>\n",
       "      <td>0.121671</td>\n",
       "      <td>0.132302</td>\n",
       "      <td>0.110974</td>\n",
       "      <td>0.035794</td>\n",
       "      <td>0.175895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.132090</td>\n",
       "      <td>0.120228</td>\n",
       "      <td>0.085113</td>\n",
       "      <td>0.095814</td>\n",
       "      <td>0.626284</td>\n",
       "      <td>0.057339</td>\n",
       "      <td>0.112206</td>\n",
       "      <td>0.275699</td>\n",
       "      <td>0.068134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.233037</td>\n",
       "      <td>0.210377</td>\n",
       "      <td>0.146233</td>\n",
       "      <td>0.153375</td>\n",
       "      <td>0.099963</td>\n",
       "      <td>0.119046</td>\n",
       "      <td>0.089955</td>\n",
       "      <td>0.107184</td>\n",
       "      <td>0.074697</td>\n",
       "      <td>0.122108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.194500</td>\n",
       "      <td>0.124396</td>\n",
       "      <td>0.122721</td>\n",
       "      <td>0.073618</td>\n",
       "      <td>0.525991</td>\n",
       "      <td>0.045205</td>\n",
       "      <td>0.104632</td>\n",
       "      <td>0.208023</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.268448</td>\n",
       "      <td>0.220253</td>\n",
       "      <td>0.137515</td>\n",
       "      <td>0.117434</td>\n",
       "      <td>0.092473</td>\n",
       "      <td>0.095937</td>\n",
       "      <td>0.092943</td>\n",
       "      <td>0.105201</td>\n",
       "      <td>0.042209</td>\n",
       "      <td>0.117400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.122438</td>\n",
       "      <td>0.120816</td>\n",
       "      <td>0.081002</td>\n",
       "      <td>0.098251</td>\n",
       "      <td>0.519534</td>\n",
       "      <td>0.057091</td>\n",
       "      <td>0.183611</td>\n",
       "      <td>0.257906</td>\n",
       "      <td>0.056574</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199547</td>\n",
       "      <td>0.191498</td>\n",
       "      <td>0.134791</td>\n",
       "      <td>0.131916</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.112010</td>\n",
       "      <td>0.101561</td>\n",
       "      <td>0.061392</td>\n",
       "      <td>0.062200</td>\n",
       "      <td>0.126947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.191578</td>\n",
       "      <td>0.120023</td>\n",
       "      <td>0.082089</td>\n",
       "      <td>0.081564</td>\n",
       "      <td>0.751143</td>\n",
       "      <td>0.065118</td>\n",
       "      <td>0.137328</td>\n",
       "      <td>0.180288</td>\n",
       "      <td>0.083758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179365</td>\n",
       "      <td>0.193517</td>\n",
       "      <td>0.128041</td>\n",
       "      <td>0.106954</td>\n",
       "      <td>0.077253</td>\n",
       "      <td>0.103637</td>\n",
       "      <td>0.106925</td>\n",
       "      <td>0.054852</td>\n",
       "      <td>0.065698</td>\n",
       "      <td>0.122043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.108925</td>\n",
       "      <td>0.182301</td>\n",
       "      <td>0.072829</td>\n",
       "      <td>0.065084</td>\n",
       "      <td>0.424973</td>\n",
       "      <td>0.060720</td>\n",
       "      <td>0.070709</td>\n",
       "      <td>0.244616</td>\n",
       "      <td>0.070471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173161</td>\n",
       "      <td>0.146510</td>\n",
       "      <td>0.099475</td>\n",
       "      <td>0.094816</td>\n",
       "      <td>0.087511</td>\n",
       "      <td>0.080800</td>\n",
       "      <td>0.075893</td>\n",
       "      <td>0.059479</td>\n",
       "      <td>0.059161</td>\n",
       "      <td>0.117966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.157950</td>\n",
       "      <td>0.143558</td>\n",
       "      <td>0.082566</td>\n",
       "      <td>0.077157</td>\n",
       "      <td>0.298469</td>\n",
       "      <td>0.063084</td>\n",
       "      <td>0.086033</td>\n",
       "      <td>0.146009</td>\n",
       "      <td>0.075223</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129042</td>\n",
       "      <td>0.181150</td>\n",
       "      <td>0.093229</td>\n",
       "      <td>0.089331</td>\n",
       "      <td>0.076168</td>\n",
       "      <td>0.078905</td>\n",
       "      <td>0.078472</td>\n",
       "      <td>0.045782</td>\n",
       "      <td>0.048674</td>\n",
       "      <td>0.082718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.133288</td>\n",
       "      <td>0.166781</td>\n",
       "      <td>0.088190</td>\n",
       "      <td>0.069939</td>\n",
       "      <td>0.534407</td>\n",
       "      <td>0.050715</td>\n",
       "      <td>0.090492</td>\n",
       "      <td>0.182010</td>\n",
       "      <td>0.035368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099805</td>\n",
       "      <td>0.153643</td>\n",
       "      <td>0.099375</td>\n",
       "      <td>0.139882</td>\n",
       "      <td>0.060916</td>\n",
       "      <td>0.086358</td>\n",
       "      <td>0.085229</td>\n",
       "      <td>0.050786</td>\n",
       "      <td>0.062157</td>\n",
       "      <td>0.088790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.104065</td>\n",
       "      <td>0.127633</td>\n",
       "      <td>0.083068</td>\n",
       "      <td>0.079099</td>\n",
       "      <td>0.349541</td>\n",
       "      <td>0.074382</td>\n",
       "      <td>0.085079</td>\n",
       "      <td>0.131658</td>\n",
       "      <td>0.045640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131747</td>\n",
       "      <td>0.180737</td>\n",
       "      <td>0.095834</td>\n",
       "      <td>0.137940</td>\n",
       "      <td>0.061104</td>\n",
       "      <td>0.100625</td>\n",
       "      <td>0.090671</td>\n",
       "      <td>0.056086</td>\n",
       "      <td>0.052517</td>\n",
       "      <td>0.117437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.111690</td>\n",
       "      <td>0.072372</td>\n",
       "      <td>0.106109</td>\n",
       "      <td>0.059135</td>\n",
       "      <td>0.319504</td>\n",
       "      <td>0.055277</td>\n",
       "      <td>0.094604</td>\n",
       "      <td>0.191622</td>\n",
       "      <td>0.031515</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089078</td>\n",
       "      <td>0.112166</td>\n",
       "      <td>0.087299</td>\n",
       "      <td>0.089427</td>\n",
       "      <td>0.074360</td>\n",
       "      <td>0.101848</td>\n",
       "      <td>0.066301</td>\n",
       "      <td>0.055298</td>\n",
       "      <td>0.066848</td>\n",
       "      <td>0.074438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.083569</td>\n",
       "      <td>0.089867</td>\n",
       "      <td>0.086618</td>\n",
       "      <td>0.055732</td>\n",
       "      <td>0.218826</td>\n",
       "      <td>0.056691</td>\n",
       "      <td>0.078445</td>\n",
       "      <td>0.256131</td>\n",
       "      <td>0.047092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109585</td>\n",
       "      <td>0.084923</td>\n",
       "      <td>0.095337</td>\n",
       "      <td>0.073974</td>\n",
       "      <td>0.058236</td>\n",
       "      <td>0.112544</td>\n",
       "      <td>0.057853</td>\n",
       "      <td>0.048497</td>\n",
       "      <td>0.057438</td>\n",
       "      <td>0.083142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.086748</td>\n",
       "      <td>0.121934</td>\n",
       "      <td>0.053831</td>\n",
       "      <td>0.056636</td>\n",
       "      <td>0.384292</td>\n",
       "      <td>0.045227</td>\n",
       "      <td>0.085217</td>\n",
       "      <td>0.129005</td>\n",
       "      <td>0.037987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109699</td>\n",
       "      <td>0.133433</td>\n",
       "      <td>0.067048</td>\n",
       "      <td>0.048642</td>\n",
       "      <td>0.040805</td>\n",
       "      <td>0.069433</td>\n",
       "      <td>0.052032</td>\n",
       "      <td>0.031520</td>\n",
       "      <td>0.042401</td>\n",
       "      <td>0.069414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.064581</td>\n",
       "      <td>0.180617</td>\n",
       "      <td>0.056246</td>\n",
       "      <td>0.046413</td>\n",
       "      <td>0.215112</td>\n",
       "      <td>0.035395</td>\n",
       "      <td>0.057443</td>\n",
       "      <td>0.103355</td>\n",
       "      <td>0.030198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065041</td>\n",
       "      <td>0.069110</td>\n",
       "      <td>0.066995</td>\n",
       "      <td>0.085021</td>\n",
       "      <td>0.039831</td>\n",
       "      <td>0.066287</td>\n",
       "      <td>0.070376</td>\n",
       "      <td>0.037808</td>\n",
       "      <td>0.050359</td>\n",
       "      <td>0.079496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.058161</td>\n",
       "      <td>0.085736</td>\n",
       "      <td>0.045026</td>\n",
       "      <td>0.045790</td>\n",
       "      <td>0.298077</td>\n",
       "      <td>0.054057</td>\n",
       "      <td>0.069617</td>\n",
       "      <td>0.104823</td>\n",
       "      <td>0.058319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071584</td>\n",
       "      <td>0.121386</td>\n",
       "      <td>0.062585</td>\n",
       "      <td>0.052825</td>\n",
       "      <td>0.036085</td>\n",
       "      <td>0.075940</td>\n",
       "      <td>0.055512</td>\n",
       "      <td>0.029622</td>\n",
       "      <td>0.046090</td>\n",
       "      <td>0.052257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.074131</td>\n",
       "      <td>0.131327</td>\n",
       "      <td>0.054687</td>\n",
       "      <td>0.055805</td>\n",
       "      <td>0.221082</td>\n",
       "      <td>0.036222</td>\n",
       "      <td>0.071743</td>\n",
       "      <td>0.158391</td>\n",
       "      <td>0.038089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077068</td>\n",
       "      <td>0.080320</td>\n",
       "      <td>0.057654</td>\n",
       "      <td>0.054775</td>\n",
       "      <td>0.045873</td>\n",
       "      <td>0.047253</td>\n",
       "      <td>0.046291</td>\n",
       "      <td>0.040640</td>\n",
       "      <td>0.066342</td>\n",
       "      <td>0.041210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.054911</td>\n",
       "      <td>0.111916</td>\n",
       "      <td>0.061446</td>\n",
       "      <td>0.042873</td>\n",
       "      <td>0.228831</td>\n",
       "      <td>0.039078</td>\n",
       "      <td>0.069045</td>\n",
       "      <td>0.130559</td>\n",
       "      <td>0.033303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112792</td>\n",
       "      <td>0.113750</td>\n",
       "      <td>0.040003</td>\n",
       "      <td>0.055393</td>\n",
       "      <td>0.045352</td>\n",
       "      <td>0.055374</td>\n",
       "      <td>0.037832</td>\n",
       "      <td>0.037670</td>\n",
       "      <td>0.042543</td>\n",
       "      <td>0.079469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.067238</td>\n",
       "      <td>0.080811</td>\n",
       "      <td>0.043024</td>\n",
       "      <td>0.044805</td>\n",
       "      <td>0.294562</td>\n",
       "      <td>0.036658</td>\n",
       "      <td>0.068371</td>\n",
       "      <td>0.112727</td>\n",
       "      <td>0.034013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082137</td>\n",
       "      <td>0.081265</td>\n",
       "      <td>0.047120</td>\n",
       "      <td>0.039982</td>\n",
       "      <td>0.038822</td>\n",
       "      <td>0.060763</td>\n",
       "      <td>0.056499</td>\n",
       "      <td>0.044012</td>\n",
       "      <td>0.039420</td>\n",
       "      <td>0.052568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.063889</td>\n",
       "      <td>0.090819</td>\n",
       "      <td>0.055873</td>\n",
       "      <td>0.048690</td>\n",
       "      <td>0.141845</td>\n",
       "      <td>0.039687</td>\n",
       "      <td>0.065689</td>\n",
       "      <td>0.070124</td>\n",
       "      <td>0.034331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069763</td>\n",
       "      <td>0.071005</td>\n",
       "      <td>0.049060</td>\n",
       "      <td>0.055379</td>\n",
       "      <td>0.038641</td>\n",
       "      <td>0.067860</td>\n",
       "      <td>0.037126</td>\n",
       "      <td>0.038117</td>\n",
       "      <td>0.027063</td>\n",
       "      <td>0.055048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.048735</td>\n",
       "      <td>0.092009</td>\n",
       "      <td>0.070164</td>\n",
       "      <td>0.054513</td>\n",
       "      <td>0.127486</td>\n",
       "      <td>0.040171</td>\n",
       "      <td>0.063679</td>\n",
       "      <td>0.105032</td>\n",
       "      <td>0.032903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081954</td>\n",
       "      <td>0.069201</td>\n",
       "      <td>0.055303</td>\n",
       "      <td>0.051275</td>\n",
       "      <td>0.047070</td>\n",
       "      <td>0.061236</td>\n",
       "      <td>0.036752</td>\n",
       "      <td>0.044466</td>\n",
       "      <td>0.041320</td>\n",
       "      <td>0.038443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.049610</td>\n",
       "      <td>0.069335</td>\n",
       "      <td>0.047966</td>\n",
       "      <td>0.048090</td>\n",
       "      <td>0.107954</td>\n",
       "      <td>0.043158</td>\n",
       "      <td>0.056713</td>\n",
       "      <td>0.124261</td>\n",
       "      <td>0.034227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089009</td>\n",
       "      <td>0.063851</td>\n",
       "      <td>0.073497</td>\n",
       "      <td>0.078834</td>\n",
       "      <td>0.078327</td>\n",
       "      <td>0.063047</td>\n",
       "      <td>0.041294</td>\n",
       "      <td>0.040600</td>\n",
       "      <td>0.045988</td>\n",
       "      <td>0.052914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.050300</td>\n",
       "      <td>0.071805</td>\n",
       "      <td>0.045320</td>\n",
       "      <td>0.061751</td>\n",
       "      <td>0.076850</td>\n",
       "      <td>0.040151</td>\n",
       "      <td>0.057186</td>\n",
       "      <td>0.160938</td>\n",
       "      <td>0.032329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077450</td>\n",
       "      <td>0.053964</td>\n",
       "      <td>0.054985</td>\n",
       "      <td>0.080580</td>\n",
       "      <td>0.048236</td>\n",
       "      <td>0.056678</td>\n",
       "      <td>0.046626</td>\n",
       "      <td>0.048921</td>\n",
       "      <td>0.033467</td>\n",
       "      <td>0.050023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.048551</td>\n",
       "      <td>0.083752</td>\n",
       "      <td>0.050863</td>\n",
       "      <td>0.045091</td>\n",
       "      <td>0.097026</td>\n",
       "      <td>0.052540</td>\n",
       "      <td>0.048083</td>\n",
       "      <td>0.076683</td>\n",
       "      <td>0.033200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057445</td>\n",
       "      <td>0.060222</td>\n",
       "      <td>0.046181</td>\n",
       "      <td>0.073013</td>\n",
       "      <td>0.052883</td>\n",
       "      <td>0.047252</td>\n",
       "      <td>0.037647</td>\n",
       "      <td>0.041325</td>\n",
       "      <td>0.030955</td>\n",
       "      <td>0.042361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.043563</td>\n",
       "      <td>0.053257</td>\n",
       "      <td>0.032412</td>\n",
       "      <td>0.047699</td>\n",
       "      <td>0.089575</td>\n",
       "      <td>0.036242</td>\n",
       "      <td>0.051227</td>\n",
       "      <td>0.068250</td>\n",
       "      <td>0.028536</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041530</td>\n",
       "      <td>0.086947</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>0.046503</td>\n",
       "      <td>0.043026</td>\n",
       "      <td>0.041851</td>\n",
       "      <td>0.032321</td>\n",
       "      <td>0.037090</td>\n",
       "      <td>0.028272</td>\n",
       "      <td>0.039495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.039320</td>\n",
       "      <td>0.066593</td>\n",
       "      <td>0.029185</td>\n",
       "      <td>0.042586</td>\n",
       "      <td>0.099253</td>\n",
       "      <td>0.032291</td>\n",
       "      <td>0.051547</td>\n",
       "      <td>0.058227</td>\n",
       "      <td>0.029355</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039074</td>\n",
       "      <td>0.041025</td>\n",
       "      <td>0.042769</td>\n",
       "      <td>0.066705</td>\n",
       "      <td>0.027132</td>\n",
       "      <td>0.041745</td>\n",
       "      <td>0.029416</td>\n",
       "      <td>0.031570</td>\n",
       "      <td>0.037014</td>\n",
       "      <td>0.044955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>0.039805</td>\n",
       "      <td>0.034974</td>\n",
       "      <td>0.029937</td>\n",
       "      <td>0.031708</td>\n",
       "      <td>0.132872</td>\n",
       "      <td>0.026981</td>\n",
       "      <td>0.069238</td>\n",
       "      <td>0.055728</td>\n",
       "      <td>0.025179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048785</td>\n",
       "      <td>0.063375</td>\n",
       "      <td>0.036606</td>\n",
       "      <td>0.048725</td>\n",
       "      <td>0.027602</td>\n",
       "      <td>0.039914</td>\n",
       "      <td>0.028436</td>\n",
       "      <td>0.041570</td>\n",
       "      <td>0.024736</td>\n",
       "      <td>0.037912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>0.062035</td>\n",
       "      <td>0.042246</td>\n",
       "      <td>0.024774</td>\n",
       "      <td>0.031327</td>\n",
       "      <td>0.094590</td>\n",
       "      <td>0.026596</td>\n",
       "      <td>0.049170</td>\n",
       "      <td>0.045944</td>\n",
       "      <td>0.022721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041769</td>\n",
       "      <td>0.051659</td>\n",
       "      <td>0.041425</td>\n",
       "      <td>0.097458</td>\n",
       "      <td>0.025308</td>\n",
       "      <td>0.037077</td>\n",
       "      <td>0.025583</td>\n",
       "      <td>0.036354</td>\n",
       "      <td>0.024314</td>\n",
       "      <td>0.026094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>0.049109</td>\n",
       "      <td>0.072258</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.043082</td>\n",
       "      <td>0.069843</td>\n",
       "      <td>0.026785</td>\n",
       "      <td>0.058743</td>\n",
       "      <td>0.042502</td>\n",
       "      <td>0.023596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033727</td>\n",
       "      <td>0.035861</td>\n",
       "      <td>0.038257</td>\n",
       "      <td>0.062804</td>\n",
       "      <td>0.025120</td>\n",
       "      <td>0.041877</td>\n",
       "      <td>0.022453</td>\n",
       "      <td>0.029662</td>\n",
       "      <td>0.021271</td>\n",
       "      <td>0.029267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>0.029572</td>\n",
       "      <td>0.074788</td>\n",
       "      <td>0.034069</td>\n",
       "      <td>0.031705</td>\n",
       "      <td>0.097347</td>\n",
       "      <td>0.062676</td>\n",
       "      <td>0.043103</td>\n",
       "      <td>0.048955</td>\n",
       "      <td>0.023321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027577</td>\n",
       "      <td>0.038118</td>\n",
       "      <td>0.039728</td>\n",
       "      <td>0.041658</td>\n",
       "      <td>0.034272</td>\n",
       "      <td>0.043172</td>\n",
       "      <td>0.029240</td>\n",
       "      <td>0.027035</td>\n",
       "      <td>0.025676</td>\n",
       "      <td>0.029891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>0.032465</td>\n",
       "      <td>0.040575</td>\n",
       "      <td>0.029591</td>\n",
       "      <td>0.028372</td>\n",
       "      <td>0.064287</td>\n",
       "      <td>0.047161</td>\n",
       "      <td>0.045591</td>\n",
       "      <td>0.048415</td>\n",
       "      <td>0.026380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030907</td>\n",
       "      <td>0.029969</td>\n",
       "      <td>0.039631</td>\n",
       "      <td>0.036126</td>\n",
       "      <td>0.022722</td>\n",
       "      <td>0.042358</td>\n",
       "      <td>0.023327</td>\n",
       "      <td>0.026049</td>\n",
       "      <td>0.019503</td>\n",
       "      <td>0.033605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>0.034170</td>\n",
       "      <td>0.034066</td>\n",
       "      <td>0.028198</td>\n",
       "      <td>0.039601</td>\n",
       "      <td>0.071141</td>\n",
       "      <td>0.019321</td>\n",
       "      <td>0.042345</td>\n",
       "      <td>0.036436</td>\n",
       "      <td>0.022686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045930</td>\n",
       "      <td>0.034009</td>\n",
       "      <td>0.039908</td>\n",
       "      <td>0.035085</td>\n",
       "      <td>0.024115</td>\n",
       "      <td>0.036553</td>\n",
       "      <td>0.022383</td>\n",
       "      <td>0.032950</td>\n",
       "      <td>0.020851</td>\n",
       "      <td>0.026953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>0.028170</td>\n",
       "      <td>0.040916</td>\n",
       "      <td>0.029239</td>\n",
       "      <td>0.030961</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.020626</td>\n",
       "      <td>0.038966</td>\n",
       "      <td>0.029323</td>\n",
       "      <td>0.021315</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024714</td>\n",
       "      <td>0.038661</td>\n",
       "      <td>0.035222</td>\n",
       "      <td>0.034487</td>\n",
       "      <td>0.017114</td>\n",
       "      <td>0.032858</td>\n",
       "      <td>0.020834</td>\n",
       "      <td>0.028725</td>\n",
       "      <td>0.018789</td>\n",
       "      <td>0.025831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>0.023245</td>\n",
       "      <td>0.034935</td>\n",
       "      <td>0.025291</td>\n",
       "      <td>0.030165</td>\n",
       "      <td>0.043993</td>\n",
       "      <td>0.020540</td>\n",
       "      <td>0.035599</td>\n",
       "      <td>0.028846</td>\n",
       "      <td>0.026392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033044</td>\n",
       "      <td>0.030979</td>\n",
       "      <td>0.028617</td>\n",
       "      <td>0.025339</td>\n",
       "      <td>0.021751</td>\n",
       "      <td>0.040477</td>\n",
       "      <td>0.023515</td>\n",
       "      <td>0.022394</td>\n",
       "      <td>0.020105</td>\n",
       "      <td>0.023515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>0.026776</td>\n",
       "      <td>0.031750</td>\n",
       "      <td>0.021042</td>\n",
       "      <td>0.034348</td>\n",
       "      <td>0.049486</td>\n",
       "      <td>0.020874</td>\n",
       "      <td>0.085697</td>\n",
       "      <td>0.036280</td>\n",
       "      <td>0.024240</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028704</td>\n",
       "      <td>0.031353</td>\n",
       "      <td>0.032053</td>\n",
       "      <td>0.056078</td>\n",
       "      <td>0.018415</td>\n",
       "      <td>0.027270</td>\n",
       "      <td>0.025112</td>\n",
       "      <td>0.021531</td>\n",
       "      <td>0.022942</td>\n",
       "      <td>0.026565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>0.032319</td>\n",
       "      <td>0.033545</td>\n",
       "      <td>0.022715</td>\n",
       "      <td>0.038901</td>\n",
       "      <td>0.068986</td>\n",
       "      <td>0.021838</td>\n",
       "      <td>0.046301</td>\n",
       "      <td>0.036459</td>\n",
       "      <td>0.024812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026675</td>\n",
       "      <td>0.025655</td>\n",
       "      <td>0.033899</td>\n",
       "      <td>0.028906</td>\n",
       "      <td>0.020504</td>\n",
       "      <td>0.038394</td>\n",
       "      <td>0.023152</td>\n",
       "      <td>0.024635</td>\n",
       "      <td>0.021144</td>\n",
       "      <td>0.026570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>0.027688</td>\n",
       "      <td>0.021541</td>\n",
       "      <td>0.029180</td>\n",
       "      <td>0.032967</td>\n",
       "      <td>0.053072</td>\n",
       "      <td>0.019655</td>\n",
       "      <td>0.042026</td>\n",
       "      <td>0.032974</td>\n",
       "      <td>0.021610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027033</td>\n",
       "      <td>0.037587</td>\n",
       "      <td>0.035728</td>\n",
       "      <td>0.043286</td>\n",
       "      <td>0.026385</td>\n",
       "      <td>0.031621</td>\n",
       "      <td>0.023485</td>\n",
       "      <td>0.023645</td>\n",
       "      <td>0.021463</td>\n",
       "      <td>0.025050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>0.024845</td>\n",
       "      <td>0.022107</td>\n",
       "      <td>0.025265</td>\n",
       "      <td>0.036777</td>\n",
       "      <td>0.045067</td>\n",
       "      <td>0.019264</td>\n",
       "      <td>0.029419</td>\n",
       "      <td>0.030220</td>\n",
       "      <td>0.035332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022595</td>\n",
       "      <td>0.032136</td>\n",
       "      <td>0.036429</td>\n",
       "      <td>0.035597</td>\n",
       "      <td>0.020911</td>\n",
       "      <td>0.027236</td>\n",
       "      <td>0.025072</td>\n",
       "      <td>0.028383</td>\n",
       "      <td>0.023541</td>\n",
       "      <td>0.022798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>0.025097</td>\n",
       "      <td>0.021226</td>\n",
       "      <td>0.026022</td>\n",
       "      <td>0.028016</td>\n",
       "      <td>0.098369</td>\n",
       "      <td>0.018371</td>\n",
       "      <td>0.031158</td>\n",
       "      <td>0.026199</td>\n",
       "      <td>0.021790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.024607</td>\n",
       "      <td>0.034423</td>\n",
       "      <td>0.028696</td>\n",
       "      <td>0.021751</td>\n",
       "      <td>0.027893</td>\n",
       "      <td>0.029108</td>\n",
       "      <td>0.024589</td>\n",
       "      <td>0.019345</td>\n",
       "      <td>0.023102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>0.021134</td>\n",
       "      <td>0.023916</td>\n",
       "      <td>0.024053</td>\n",
       "      <td>0.024933</td>\n",
       "      <td>0.073267</td>\n",
       "      <td>0.020776</td>\n",
       "      <td>0.031802</td>\n",
       "      <td>0.025496</td>\n",
       "      <td>0.020281</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021904</td>\n",
       "      <td>0.030380</td>\n",
       "      <td>0.032025</td>\n",
       "      <td>0.025125</td>\n",
       "      <td>0.019324</td>\n",
       "      <td>0.027438</td>\n",
       "      <td>0.021064</td>\n",
       "      <td>0.029352</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.023844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>0.019697</td>\n",
       "      <td>0.020916</td>\n",
       "      <td>0.022500</td>\n",
       "      <td>0.026673</td>\n",
       "      <td>0.050287</td>\n",
       "      <td>0.017520</td>\n",
       "      <td>0.027514</td>\n",
       "      <td>0.024306</td>\n",
       "      <td>0.017694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021832</td>\n",
       "      <td>0.020317</td>\n",
       "      <td>0.032259</td>\n",
       "      <td>0.026103</td>\n",
       "      <td>0.017044</td>\n",
       "      <td>0.026956</td>\n",
       "      <td>0.018770</td>\n",
       "      <td>0.028755</td>\n",
       "      <td>0.017814</td>\n",
       "      <td>0.022692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>0.019131</td>\n",
       "      <td>0.020624</td>\n",
       "      <td>0.020862</td>\n",
       "      <td>0.029064</td>\n",
       "      <td>0.044341</td>\n",
       "      <td>0.017373</td>\n",
       "      <td>0.026495</td>\n",
       "      <td>0.025270</td>\n",
       "      <td>0.017868</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019895</td>\n",
       "      <td>0.027838</td>\n",
       "      <td>0.035257</td>\n",
       "      <td>0.026953</td>\n",
       "      <td>0.016119</td>\n",
       "      <td>0.023699</td>\n",
       "      <td>0.018585</td>\n",
       "      <td>0.023623</td>\n",
       "      <td>0.016659</td>\n",
       "      <td>0.021311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>0.019246</td>\n",
       "      <td>0.020611</td>\n",
       "      <td>0.022137</td>\n",
       "      <td>0.033668</td>\n",
       "      <td>0.033199</td>\n",
       "      <td>0.018957</td>\n",
       "      <td>0.026564</td>\n",
       "      <td>0.026267</td>\n",
       "      <td>0.019020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019477</td>\n",
       "      <td>0.021080</td>\n",
       "      <td>0.030123</td>\n",
       "      <td>0.025294</td>\n",
       "      <td>0.018579</td>\n",
       "      <td>0.022949</td>\n",
       "      <td>0.017646</td>\n",
       "      <td>0.022485</td>\n",
       "      <td>0.020071</td>\n",
       "      <td>0.019849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>0.019729</td>\n",
       "      <td>0.024535</td>\n",
       "      <td>0.020011</td>\n",
       "      <td>0.021467</td>\n",
       "      <td>0.031175</td>\n",
       "      <td>0.016859</td>\n",
       "      <td>0.026571</td>\n",
       "      <td>0.024018</td>\n",
       "      <td>0.018719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.019164</td>\n",
       "      <td>0.030339</td>\n",
       "      <td>0.025478</td>\n",
       "      <td>0.017242</td>\n",
       "      <td>0.022453</td>\n",
       "      <td>0.017550</td>\n",
       "      <td>0.021562</td>\n",
       "      <td>0.016064</td>\n",
       "      <td>0.019209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>0.018766</td>\n",
       "      <td>0.019081</td>\n",
       "      <td>0.017708</td>\n",
       "      <td>0.032281</td>\n",
       "      <td>0.029849</td>\n",
       "      <td>0.016423</td>\n",
       "      <td>0.023562</td>\n",
       "      <td>0.024745</td>\n",
       "      <td>0.017896</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018408</td>\n",
       "      <td>0.017601</td>\n",
       "      <td>0.030387</td>\n",
       "      <td>0.023253</td>\n",
       "      <td>0.015824</td>\n",
       "      <td>0.022137</td>\n",
       "      <td>0.017506</td>\n",
       "      <td>0.021006</td>\n",
       "      <td>0.015327</td>\n",
       "      <td>0.017396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.015821</td>\n",
       "      <td>0.018020</td>\n",
       "      <td>0.024272</td>\n",
       "      <td>0.027274</td>\n",
       "      <td>0.016188</td>\n",
       "      <td>0.024640</td>\n",
       "      <td>0.026430</td>\n",
       "      <td>0.019139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017717</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022932</td>\n",
       "      <td>0.016002</td>\n",
       "      <td>0.021456</td>\n",
       "      <td>0.018205</td>\n",
       "      <td>0.021593</td>\n",
       "      <td>0.017473</td>\n",
       "      <td>0.018307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0         A         R         N         D         C         Q  \\\n",
       "0            0  0.215069  0.483352  0.143512  0.133588  2.217512  0.130810   \n",
       "1            1  0.114678  0.429829  0.103611  0.101690  2.369031  0.075304   \n",
       "2            2  0.160948  0.241044  0.097990  0.099774  0.765575  0.088194   \n",
       "3            3  0.163472  0.423496  0.105217  0.068877  1.163470  0.076649   \n",
       "4            4  0.070393  0.165129  0.092374  0.078504  1.478498  0.075313   \n",
       "5            5  0.132378  0.137835  0.086295  0.102439  0.446971  0.039611   \n",
       "6            6  0.160407  0.089654  0.079356  0.113461  0.372536  0.050125   \n",
       "7            7  0.132090  0.120228  0.085113  0.095814  0.626284  0.057339   \n",
       "8            8  0.194500  0.124396  0.122721  0.073618  0.525991  0.045205   \n",
       "9            9  0.122438  0.120816  0.081002  0.098251  0.519534  0.057091   \n",
       "10          10  0.191578  0.120023  0.082089  0.081564  0.751143  0.065118   \n",
       "11          11  0.108925  0.182301  0.072829  0.065084  0.424973  0.060720   \n",
       "12          12  0.157950  0.143558  0.082566  0.077157  0.298469  0.063084   \n",
       "13          13  0.133288  0.166781  0.088190  0.069939  0.534407  0.050715   \n",
       "14          14  0.104065  0.127633  0.083068  0.079099  0.349541  0.074382   \n",
       "15          15  0.111690  0.072372  0.106109  0.059135  0.319504  0.055277   \n",
       "16          16  0.083569  0.089867  0.086618  0.055732  0.218826  0.056691   \n",
       "17          17  0.086748  0.121934  0.053831  0.056636  0.384292  0.045227   \n",
       "18          18  0.064581  0.180617  0.056246  0.046413  0.215112  0.035395   \n",
       "19          19  0.058161  0.085736  0.045026  0.045790  0.298077  0.054057   \n",
       "20          20  0.074131  0.131327  0.054687  0.055805  0.221082  0.036222   \n",
       "21          21  0.054911  0.111916  0.061446  0.042873  0.228831  0.039078   \n",
       "22          22  0.067238  0.080811  0.043024  0.044805  0.294562  0.036658   \n",
       "23          23  0.063889  0.090819  0.055873  0.048690  0.141845  0.039687   \n",
       "24          24  0.048735  0.092009  0.070164  0.054513  0.127486  0.040171   \n",
       "25          25  0.049610  0.069335  0.047966  0.048090  0.107954  0.043158   \n",
       "26          26  0.050300  0.071805  0.045320  0.061751  0.076850  0.040151   \n",
       "27          27  0.048551  0.083752  0.050863  0.045091  0.097026  0.052540   \n",
       "28          28  0.043563  0.053257  0.032412  0.047699  0.089575  0.036242   \n",
       "29          29  0.039320  0.066593  0.029185  0.042586  0.099253  0.032291   \n",
       "30          30  0.039805  0.034974  0.029937  0.031708  0.132872  0.026981   \n",
       "31          31  0.062035  0.042246  0.024774  0.031327  0.094590  0.026596   \n",
       "32          32  0.049109  0.072258  0.025391  0.043082  0.069843  0.026785   \n",
       "33          33  0.029572  0.074788  0.034069  0.031705  0.097347  0.062676   \n",
       "34          34  0.032465  0.040575  0.029591  0.028372  0.064287  0.047161   \n",
       "35          35  0.034170  0.034066  0.028198  0.039601  0.071141  0.019321   \n",
       "36          36  0.028170  0.040916  0.029239  0.030961  0.063830  0.020626   \n",
       "37          37  0.023245  0.034935  0.025291  0.030165  0.043993  0.020540   \n",
       "38          38  0.026776  0.031750  0.021042  0.034348  0.049486  0.020874   \n",
       "39          39  0.032319  0.033545  0.022715  0.038901  0.068986  0.021838   \n",
       "40          40  0.027688  0.021541  0.029180  0.032967  0.053072  0.019655   \n",
       "41          41  0.024845  0.022107  0.025265  0.036777  0.045067  0.019264   \n",
       "42          42  0.025097  0.021226  0.026022  0.028016  0.098369  0.018371   \n",
       "43          43  0.021134  0.023916  0.024053  0.024933  0.073267  0.020776   \n",
       "44          44  0.019697  0.020916  0.022500  0.026673  0.050287  0.017520   \n",
       "45          45  0.019131  0.020624  0.020862  0.029064  0.044341  0.017373   \n",
       "46          46  0.019246  0.020611  0.022137  0.033668  0.033199  0.018957   \n",
       "47          47  0.019729  0.024535  0.020011  0.021467  0.031175  0.016859   \n",
       "48          48  0.018766  0.019081  0.017708  0.032281  0.029849  0.016423   \n",
       "49          49  0.018182  0.015821  0.018020  0.024272  0.027274  0.016188   \n",
       "\n",
       "           E         G         H  ...         L         K         M         F  \\\n",
       "0   0.168565  0.715721  0.064815  ...  0.428184  0.316324  0.184180  0.214149   \n",
       "1   0.168214  0.340382  0.104869  ...  0.360381  0.416033  0.224845  0.165256   \n",
       "2   0.103967  0.402441  0.087890  ...  0.305895  0.244981  0.131540  0.123376   \n",
       "3   0.104508  0.201092  0.077516  ...  0.346661  0.286027  0.160886  0.129036   \n",
       "4   0.106306  0.249911  0.077612  ...  0.249818  0.137413  0.138184  0.150049   \n",
       "5   0.085510  0.320189  0.061554  ...  0.225524  0.181738  0.120943  0.121934   \n",
       "6   0.064223  0.205711  0.082409  ...  0.177923  0.236984  0.142232  0.132447   \n",
       "7   0.112206  0.275699  0.068134  ...  0.233037  0.210377  0.146233  0.153375   \n",
       "8   0.104632  0.208023  0.054902  ...  0.268448  0.220253  0.137515  0.117434   \n",
       "9   0.183611  0.257906  0.056574  ...  0.199547  0.191498  0.134791  0.131916   \n",
       "10  0.137328  0.180288  0.083758  ...  0.179365  0.193517  0.128041  0.106954   \n",
       "11  0.070709  0.244616  0.070471  ...  0.173161  0.146510  0.099475  0.094816   \n",
       "12  0.086033  0.146009  0.075223  ...  0.129042  0.181150  0.093229  0.089331   \n",
       "13  0.090492  0.182010  0.035368  ...  0.099805  0.153643  0.099375  0.139882   \n",
       "14  0.085079  0.131658  0.045640  ...  0.131747  0.180737  0.095834  0.137940   \n",
       "15  0.094604  0.191622  0.031515  ...  0.089078  0.112166  0.087299  0.089427   \n",
       "16  0.078445  0.256131  0.047092  ...  0.109585  0.084923  0.095337  0.073974   \n",
       "17  0.085217  0.129005  0.037987  ...  0.109699  0.133433  0.067048  0.048642   \n",
       "18  0.057443  0.103355  0.030198  ...  0.065041  0.069110  0.066995  0.085021   \n",
       "19  0.069617  0.104823  0.058319  ...  0.071584  0.121386  0.062585  0.052825   \n",
       "20  0.071743  0.158391  0.038089  ...  0.077068  0.080320  0.057654  0.054775   \n",
       "21  0.069045  0.130559  0.033303  ...  0.112792  0.113750  0.040003  0.055393   \n",
       "22  0.068371  0.112727  0.034013  ...  0.082137  0.081265  0.047120  0.039982   \n",
       "23  0.065689  0.070124  0.034331  ...  0.069763  0.071005  0.049060  0.055379   \n",
       "24  0.063679  0.105032  0.032903  ...  0.081954  0.069201  0.055303  0.051275   \n",
       "25  0.056713  0.124261  0.034227  ...  0.089009  0.063851  0.073497  0.078834   \n",
       "26  0.057186  0.160938  0.032329  ...  0.077450  0.053964  0.054985  0.080580   \n",
       "27  0.048083  0.076683  0.033200  ...  0.057445  0.060222  0.046181  0.073013   \n",
       "28  0.051227  0.068250  0.028536  ...  0.041530  0.086947  0.046800  0.046503   \n",
       "29  0.051547  0.058227  0.029355  ...  0.039074  0.041025  0.042769  0.066705   \n",
       "30  0.069238  0.055728  0.025179  ...  0.048785  0.063375  0.036606  0.048725   \n",
       "31  0.049170  0.045944  0.022721  ...  0.041769  0.051659  0.041425  0.097458   \n",
       "32  0.058743  0.042502  0.023596  ...  0.033727  0.035861  0.038257  0.062804   \n",
       "33  0.043103  0.048955  0.023321  ...  0.027577  0.038118  0.039728  0.041658   \n",
       "34  0.045591  0.048415  0.026380  ...  0.030907  0.029969  0.039631  0.036126   \n",
       "35  0.042345  0.036436  0.022686  ...  0.045930  0.034009  0.039908  0.035085   \n",
       "36  0.038966  0.029323  0.021315  ...  0.024714  0.038661  0.035222  0.034487   \n",
       "37  0.035599  0.028846  0.026392  ...  0.033044  0.030979  0.028617  0.025339   \n",
       "38  0.085697  0.036280  0.024240  ...  0.028704  0.031353  0.032053  0.056078   \n",
       "39  0.046301  0.036459  0.024812  ...  0.026675  0.025655  0.033899  0.028906   \n",
       "40  0.042026  0.032974  0.021610  ...  0.027033  0.037587  0.035728  0.043286   \n",
       "41  0.029419  0.030220  0.035332  ...  0.022595  0.032136  0.036429  0.035597   \n",
       "42  0.031158  0.026199  0.021790  ...  0.024390  0.024607  0.034423  0.028696   \n",
       "43  0.031802  0.025496  0.020281  ...  0.021904  0.030380  0.032025  0.025125   \n",
       "44  0.027514  0.024306  0.017694  ...  0.021832  0.020317  0.032259  0.026103   \n",
       "45  0.026495  0.025270  0.017868  ...  0.019895  0.027838  0.035257  0.026953   \n",
       "46  0.026564  0.026267  0.019020  ...  0.019477  0.021080  0.030123  0.025294   \n",
       "47  0.026571  0.024018  0.018719  ...  0.018868  0.019164  0.030339  0.025478   \n",
       "48  0.023562  0.024745  0.017896  ...  0.018408  0.017601  0.030387  0.023253   \n",
       "49  0.024640  0.026430  0.019139  ...  0.017717  0.015797  0.000000  0.022932   \n",
       "\n",
       "           P         S         T         W         Y         V  \n",
       "0   0.257585  0.225357  0.148889  0.143170  0.103946  0.182000  \n",
       "1   0.205752  0.227474  0.128598  0.096066  0.107744  0.189875  \n",
       "2   0.159369  0.174201  0.158651  0.104443  0.097218  0.247258  \n",
       "3   0.131611  0.135475  0.142005  0.164302  0.096828  0.122887  \n",
       "4   0.135389  0.133322  0.130349  0.101665  0.033102  0.104636  \n",
       "5   0.087696  0.076736  0.132841  0.107809  0.038939  0.098509  \n",
       "6   0.062499  0.121671  0.132302  0.110974  0.035794  0.175895  \n",
       "7   0.099963  0.119046  0.089955  0.107184  0.074697  0.122108  \n",
       "8   0.092473  0.095937  0.092943  0.105201  0.042209  0.117400  \n",
       "9   0.115607  0.112010  0.101561  0.061392  0.062200  0.126947  \n",
       "10  0.077253  0.103637  0.106925  0.054852  0.065698  0.122043  \n",
       "11  0.087511  0.080800  0.075893  0.059479  0.059161  0.117966  \n",
       "12  0.076168  0.078905  0.078472  0.045782  0.048674  0.082718  \n",
       "13  0.060916  0.086358  0.085229  0.050786  0.062157  0.088790  \n",
       "14  0.061104  0.100625  0.090671  0.056086  0.052517  0.117437  \n",
       "15  0.074360  0.101848  0.066301  0.055298  0.066848  0.074438  \n",
       "16  0.058236  0.112544  0.057853  0.048497  0.057438  0.083142  \n",
       "17  0.040805  0.069433  0.052032  0.031520  0.042401  0.069414  \n",
       "18  0.039831  0.066287  0.070376  0.037808  0.050359  0.079496  \n",
       "19  0.036085  0.075940  0.055512  0.029622  0.046090  0.052257  \n",
       "20  0.045873  0.047253  0.046291  0.040640  0.066342  0.041210  \n",
       "21  0.045352  0.055374  0.037832  0.037670  0.042543  0.079469  \n",
       "22  0.038822  0.060763  0.056499  0.044012  0.039420  0.052568  \n",
       "23  0.038641  0.067860  0.037126  0.038117  0.027063  0.055048  \n",
       "24  0.047070  0.061236  0.036752  0.044466  0.041320  0.038443  \n",
       "25  0.078327  0.063047  0.041294  0.040600  0.045988  0.052914  \n",
       "26  0.048236  0.056678  0.046626  0.048921  0.033467  0.050023  \n",
       "27  0.052883  0.047252  0.037647  0.041325  0.030955  0.042361  \n",
       "28  0.043026  0.041851  0.032321  0.037090  0.028272  0.039495  \n",
       "29  0.027132  0.041745  0.029416  0.031570  0.037014  0.044955  \n",
       "30  0.027602  0.039914  0.028436  0.041570  0.024736  0.037912  \n",
       "31  0.025308  0.037077  0.025583  0.036354  0.024314  0.026094  \n",
       "32  0.025120  0.041877  0.022453  0.029662  0.021271  0.029267  \n",
       "33  0.034272  0.043172  0.029240  0.027035  0.025676  0.029891  \n",
       "34  0.022722  0.042358  0.023327  0.026049  0.019503  0.033605  \n",
       "35  0.024115  0.036553  0.022383  0.032950  0.020851  0.026953  \n",
       "36  0.017114  0.032858  0.020834  0.028725  0.018789  0.025831  \n",
       "37  0.021751  0.040477  0.023515  0.022394  0.020105  0.023515  \n",
       "38  0.018415  0.027270  0.025112  0.021531  0.022942  0.026565  \n",
       "39  0.020504  0.038394  0.023152  0.024635  0.021144  0.026570  \n",
       "40  0.026385  0.031621  0.023485  0.023645  0.021463  0.025050  \n",
       "41  0.020911  0.027236  0.025072  0.028383  0.023541  0.022798  \n",
       "42  0.021751  0.027893  0.029108  0.024589  0.019345  0.023102  \n",
       "43  0.019324  0.027438  0.021064  0.029352  0.017857  0.023844  \n",
       "44  0.017044  0.026956  0.018770  0.028755  0.017814  0.022692  \n",
       "45  0.016119  0.023699  0.018585  0.023623  0.016659  0.021311  \n",
       "46  0.018579  0.022949  0.017646  0.022485  0.020071  0.019849  \n",
       "47  0.017242  0.022453  0.017550  0.021562  0.016064  0.019209  \n",
       "48  0.015824  0.022137  0.017506  0.021006  0.015327  0.017396  \n",
       "49  0.016002  0.021456  0.018205  0.021593  0.017473  0.018307  \n",
       "\n",
       "[50 rows x 21 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contribs = pd.read_csv('results/contribs/average_contribs.csv')\n",
    "contribs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Position Top 1 Top 2 Top 3 Top 4\n",
      "0          0     C     G     R     L\n",
      "1          1     C     R     K     L\n",
      "2          2     C     G     L     V\n",
      "3          3     C     R     L     K\n",
      "4          4     C     G     L     R\n",
      "5          5     C     G     L     K\n",
      "6          6     C     K     G     L\n",
      "7          7     C     G     L     K\n",
      "8          8     C     L     K     G\n",
      "9          9     C     G     L     K\n",
      "10        10     C     K     A     G\n",
      "11        11     C     G     R     L\n",
      "12        12     C     K     A     G\n",
      "13        13     C     G     R     K\n",
      "14        14     C     K     F     L\n",
      "15        15     C     G     K     A\n",
      "16        16     G     C     I     S\n",
      "17        17     C     K     G     R\n",
      "18        18     C     R     G     F\n",
      "19        19     C     K     G     R\n",
      "20        20     C     G     R     K\n",
      "21        21     C     G     K     L\n",
      "22        22     C     G     L     K\n",
      "23        23     C     R     K     G\n",
      "24        24     C     G     R     L\n",
      "25        25     G     C     L     F\n",
      "26        26     G     F     L     C\n",
      "27        27     C     R     G     F\n",
      "28        28     C     K     G     R\n",
      "29        29     C     F     R     G\n",
      "30        30     C     E     K     G\n",
      "31        31     F     C     A     K\n",
      "32        32     R     C     F     E\n",
      "33        33     C     R     Q     G\n",
      "34        34     C     G     Q     E\n",
      "35        35     C     L     E     M\n",
      "36        36     C     R     E     K\n",
      "37        37     C     S     E     R\n",
      "38        38     E     F     C     G\n",
      "39        39     C     E     D     S\n",
      "40        40     C     F     E     K\n",
      "41        41     C     D     M     F\n",
      "42        42     C     M     E     T\n",
      "43        43     C     M     E     K\n",
      "44        44     C     M     W     E\n",
      "45        45     C     M     D     K\n",
      "46        46     D     C     M     E\n",
      "47        47     C     M     E     F\n",
      "48        48     D     M     C     G\n",
      "49        49     C     G     E     D\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "contribs = pd.read_csv('results/contribs/average_contribs.csv')\n",
    "\n",
    "# Initialize a list to store the top 3 amino acids for each position\n",
    "top_amino_acids = []\n",
    "\n",
    "# Iterate through each position (row) in the DataFrame\n",
    "for index, row in contribs.iterrows():\n",
    "    # Get the row as a dictionary and remove the 'Unnamed: 0' key\n",
    "    row_dict = row.to_dict()\n",
    "    del row_dict['Unnamed: 0']\n",
    "    \n",
    "    # Sort the dictionary by values in descending order and get the top 3 items\n",
    "    sorted_amino_acids = sorted(row_dict.items(), key=lambda x: x[1], reverse=True)[:4]\n",
    "    \n",
    "    # Extract only the amino acid names (keys) and add to the list\n",
    "    top_amino_acids.append([index] + [aa[0] for aa in sorted_amino_acids])\n",
    "\n",
    "# Convert the list to a DataFrame for better readability\n",
    "top_amino_acids_df = pd.DataFrame(top_amino_acids, columns=['Position', 'Top 1', 'Top 2', 'Top 3', 'Top 4'])\n",
    "\n",
    "# Print the DataFrame\n",
    "print(top_amino_acids_df)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "top_amino_acids_df.to_csv('top_amino_acids.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = 'model/model_layer1_multihead9_fold1_new.pkl'\n",
    "model_eval = Transformer().to(device)\n",
    "# u state dictionaryju su obično pohranjeni weights i biases\n",
    "model_eval.load_state_dict(torch.load(model_file, map_location='cpu'), strict = True)\n",
    "\n",
    "state_dict_keys = model_eval.state_dict().keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate_sequence(sequence):\n",
    "    print('----------')\n",
    "    top_aa = pd.read_csv('top_amino_acids.csv')\n",
    "    print(top_aa['Top 1'][0])\n",
    "    ls = []\n",
    "    \n",
    "    for i in range(len(sequence)):\n",
    "        seq_list = list(sequence)  # Convert sequence to a list of characters\n",
    "        seq_list[i] = top_aa['Top 1'][i]\n",
    "        mutated_sequence = ''.join(seq_list)  # Join the list back into a string\n",
    "        ls.append(mutated_sequence)\n",
    "        \n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n",
      "['CEP', 'GCP', 'GEC']\n",
      "tn:\n",
      "       Unnamed: 0                                           sequence  label  \\\n",
      "0              0                                                GEP      0   \n",
      "1              1                                                QHP      0   \n",
      "2              2                                               YLRF      0   \n",
      "3              3                                               FMRF      0   \n",
      "4              9                                               TKPR      0   \n",
      "...          ...                                                ...    ...   \n",
      "5175        9563  MAREGFTLECTSCKMQNYISKKNKKLHPEKVKLSKYCSKCSKHSVH...      0   \n",
      "5176        9564  MISMLRCTFFFVSVILITSYFVTPTMSIKCNRKRHVIKPHICRKIC...      0   \n",
      "5177        9569  MKQTFQPSNRKRKNKHGFRSRMKTINGRRILASRRAKGRKKLTVSD...      0   \n",
      "5178        9570  MGSPEKLRPSDFSKSFLISSIRFAMSFSSFELYSACSSLIRVSSPT...      0   \n",
      "5179        9571  MLLPATMSDKPDMAEIEKFDKSKLKKTETQEKNPLPSKETIEQEKQ...      0   \n",
      "\n",
      "                                                peptide  y_pred  y_prob  \n",
      "0                                                   GEP       0  0.1166  \n",
      "1                                                   QHP       0  0.0842  \n",
      "2                                                  YLRF       0  0.2936  \n",
      "3                                                  FMRF       0  0.4007  \n",
      "4                                                  TKPR       0  0.3111  \n",
      "...                                                 ...     ...     ...  \n",
      "5175  MAREGFTLECTSCKMQNYISKKNKKLHPEKVKLSKYCSKCSKHSVH...       0  0.0034  \n",
      "5176  MISMLRCTFFFVSVILITSYFVTPTMSIKCNRKRHVIKPHICRKIC...       0  0.0420  \n",
      "5177  MKQTFQPSNRKRKNKHGFRSRMKTINGRRILASRRAKGRKKLTVSD...       0  0.0015  \n",
      "5178  MGSPEKLRPSDFSKSFLISSIRFAMSFSSFELYSACSSLIRVSSPT...       0  0.0022  \n",
      "5179  MLLPATMSDKPDMAEIEKFDKSKLKKTETQEKNPLPSKETIEQEKQ...       0  0.0004  \n",
      "\n",
      "[5180 rows x 6 columns]\n",
      "                                                peptide\n",
      "0                                                   GEP\n",
      "1                                                   QHP\n",
      "2                                                  YLRF\n",
      "3                                                  FMRF\n",
      "4                                                  TKPR\n",
      "...                                                 ...\n",
      "5175  MAREGFTLECTSCKMQNYISKKNKKLHPEKVKLSKYCSKCSKHSVH...\n",
      "5176  MISMLRCTFFFVSVILITSYFVTPTMSIKCNRKRHVIKPHICRKIC...\n",
      "5177  MKQTFQPSNRKRKNKHGFRSRMKTINGRRILASRRAKGRKKLTVSD...\n",
      "5178  MGSPEKLRPSDFSKSFLISSIRFAMSFSSFELYSACSSLIRVSSPT...\n",
      "5179  MLLPATMSDKPDMAEIEKFDKSKLKKTETQEKNPLPSKETIEQEKQ...\n",
      "\n",
      "[5180 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "true_negative_samples = pd.read_csv(\"true_negative_samples.csv\")\n",
    "\n",
    "print(mutate_sequence(true_negative_samples['sequence'][0]))\n",
    "\n",
    "print(\"tn:\\n\", true_negative_samples)\n",
    "\n",
    "# print(peptides)\n",
    "predict_data = pd.DataFrame([true_negative_samples['sequence']], index = ['peptide']).T\n",
    "print(predict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max mut =  1\n",
      "  peptide\n",
      "0     CEP\n",
      "1     GEP\n",
      "2     REP\n",
      "3     LEP\n",
      "4     GCP\n",
      "5     GRP\n",
      "6     GKP\n",
      "7     GLP\n",
      "8     GEC\n",
      "9     GEG\n",
      "# Samples =  12\n",
      "val_loader length: 1\n",
      "val_pep_inputs: torch.Size([12, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([12, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([12, 9, 50, 50])\n",
      "dec_self_attn_pad_mask shape: torch.Size([12, 50, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([12, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([12, 9, 50, 50])\n",
      "dec_self_attn shape in decoder layer: 12 torch.Size([9, 50, 50])\n",
      "dec_self_attns shape in decoder: 1 torch.Size([12, 9, 50, 50])\n",
      "dec_self_attns shape in transformer: 1 torch.Size([12, 9, 50, 50])\n",
      "val_dec_self_attns shape: 1 torch.Size([12, 9, 50, 50])\n",
      "max mut =  1\n",
      "  peptide\n",
      "0     CHP\n",
      "1     GHP\n",
      "2     RHP\n",
      "3     LHP\n",
      "4     QCP\n",
      "5     QRP\n",
      "6     QKP\n",
      "7     QLP\n",
      "8     QHC\n",
      "9     QHG\n",
      "# Samples =  12\n",
      "val_loader length: 1\n",
      "val_pep_inputs: torch.Size([12, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([12, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([12, 9, 50, 50])\n",
      "dec_self_attn_pad_mask shape: torch.Size([12, 50, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([12, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([12, 9, 50, 50])\n",
      "dec_self_attn shape in decoder layer: 12 torch.Size([9, 50, 50])\n",
      "dec_self_attns shape in decoder: 1 torch.Size([12, 9, 50, 50])\n",
      "dec_self_attns shape in transformer: 1 torch.Size([12, 9, 50, 50])\n",
      "val_dec_self_attns shape: 1 torch.Size([12, 9, 50, 50])\n",
      "max mut =  1\n",
      "  peptide\n",
      "0    CLRF\n",
      "1    GLRF\n",
      "2    RLRF\n",
      "3    LLRF\n",
      "4    YCRF\n",
      "5    YRRF\n",
      "6    YKRF\n",
      "7    YLRF\n",
      "8    YLCF\n",
      "9    YLGF\n",
      "# Samples =  16\n",
      "val_loader length: 1\n",
      "val_pep_inputs: torch.Size([16, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([16, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([16, 9, 50, 50])\n",
      "dec_self_attn_pad_mask shape: torch.Size([16, 50, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([16, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([16, 9, 50, 50])\n",
      "dec_self_attn shape in decoder layer: 16 torch.Size([9, 50, 50])\n",
      "dec_self_attns shape in decoder: 1 torch.Size([16, 9, 50, 50])\n",
      "dec_self_attns shape in transformer: 1 torch.Size([16, 9, 50, 50])\n",
      "val_dec_self_attns shape: 1 torch.Size([16, 9, 50, 50])\n",
      "max mut =  1\n",
      "  peptide\n",
      "0    CMRF\n",
      "1    GMRF\n",
      "2    RMRF\n",
      "3    LMRF\n",
      "4    FCRF\n",
      "5    FRRF\n",
      "6    FKRF\n",
      "7    FLRF\n",
      "8    FMCF\n",
      "9    FMGF\n",
      "# Samples =  16\n",
      "val_loader length: 1\n",
      "val_pep_inputs: torch.Size([16, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([16, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([16, 9, 50, 50])\n",
      "dec_self_attn_pad_mask shape: torch.Size([16, 50, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([16, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([16, 9, 50, 50])\n",
      "dec_self_attn shape in decoder layer: 16 torch.Size([9, 50, 50])\n",
      "dec_self_attns shape in decoder: 1 torch.Size([16, 9, 50, 50])\n",
      "dec_self_attns shape in transformer: 1 torch.Size([16, 9, 50, 50])\n",
      "val_dec_self_attns shape: 1 torch.Size([16, 9, 50, 50])\n",
      "max mut =  1\n",
      "  peptide\n",
      "0    CKPR\n",
      "1    GKPR\n",
      "2    RKPR\n",
      "3    LKPR\n",
      "4    TCPR\n",
      "5    TRPR\n",
      "6    TKPR\n",
      "7    TLPR\n",
      "8    TKCR\n",
      "9    TKGR\n",
      "# Samples =  16\n",
      "val_loader length: 1\n",
      "val_pep_inputs: torch.Size([16, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([16, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([16, 9, 50, 50])\n",
      "dec_self_attn_pad_mask shape: torch.Size([16, 50, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([16, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([16, 9, 50, 50])\n",
      "dec_self_attn shape in decoder layer: 16 torch.Size([9, 50, 50])\n",
      "dec_self_attns shape in decoder: 1 torch.Size([16, 9, 50, 50])\n",
      "dec_self_attns shape in transformer: 1 torch.Size([16, 9, 50, 50])\n",
      "val_dec_self_attns shape: 1 torch.Size([16, 9, 50, 50])\n",
      "max mut =  1\n",
      "  peptide\n",
      "0    CFGD\n",
      "1    GFGD\n",
      "2    RFGD\n",
      "3    LFGD\n",
      "4    GCGD\n",
      "5    GRGD\n",
      "6    GKGD\n",
      "7    GLGD\n",
      "8    GFCD\n",
      "9    GFGD\n",
      "# Samples =  16\n",
      "val_loader length: 1\n",
      "val_pep_inputs: torch.Size([16, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([16, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([16, 9, 50, 50])\n",
      "dec_self_attn_pad_mask shape: torch.Size([16, 50, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([16, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([16, 9, 50, 50])\n",
      "dec_self_attn shape in decoder layer: 16 torch.Size([9, 50, 50])\n",
      "dec_self_attns shape in decoder: 1 torch.Size([16, 9, 50, 50])\n",
      "dec_self_attns shape in transformer: 1 torch.Size([16, 9, 50, 50])\n",
      "val_dec_self_attns shape: 1 torch.Size([16, 9, 50, 50])\n",
      "max mut =  1\n",
      "  peptide\n",
      "0    CFAD\n",
      "1    GFAD\n",
      "2    RFAD\n",
      "3    LFAD\n",
      "4    GCAD\n",
      "5    GRAD\n",
      "6    GKAD\n",
      "7    GLAD\n",
      "8    GFCD\n",
      "9    GFGD\n",
      "# Samples =  16\n",
      "val_loader length: 1\n",
      "val_pep_inputs: torch.Size([16, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([16, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([16, 9, 50, 50])\n",
      "dec_self_attn_pad_mask shape: torch.Size([16, 50, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([16, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([16, 9, 50, 50])\n",
      "dec_self_attn shape in decoder layer: 16 torch.Size([9, 50, 50])\n",
      "dec_self_attns shape in decoder: 1 torch.Size([16, 9, 50, 50])\n",
      "dec_self_attns shape in transformer: 1 torch.Size([16, 9, 50, 50])\n",
      "val_dec_self_attns shape: 1 torch.Size([16, 9, 50, 50])\n",
      "max mut =  1\n",
      "  peptide\n",
      "0    CSWD\n",
      "1    GSWD\n",
      "2    RSWD\n",
      "3    LSWD\n",
      "4    GCWD\n",
      "5    GRWD\n",
      "6    GKWD\n",
      "7    GLWD\n",
      "8    GSCD\n",
      "9    GSGD\n",
      "# Samples =  16\n",
      "val_loader length: 1\n",
      "val_pep_inputs: torch.Size([16, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([16, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([16, 9, 50, 50])\n",
      "dec_self_attn_pad_mask shape: torch.Size([16, 50, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([16, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([16, 9, 50, 50])\n",
      "dec_self_attn shape in decoder layer: 16 torch.Size([9, 50, 50])\n",
      "dec_self_attns shape in decoder: 1 torch.Size([16, 9, 50, 50])\n",
      "dec_self_attns shape in transformer: 1 torch.Size([16, 9, 50, 50])\n",
      "val_dec_self_attns shape: 1 torch.Size([16, 9, 50, 50])\n",
      "max mut =  1\n",
      "  peptide\n",
      "0    CTGG\n",
      "1    GTGG\n",
      "2    RTGG\n",
      "3    LTGG\n",
      "4    GCGG\n",
      "5    GRGG\n",
      "6    GKGG\n",
      "7    GLGG\n",
      "8    GTCG\n",
      "9    GTGG\n",
      "# Samples =  16\n",
      "val_loader length: 1\n",
      "val_pep_inputs: torch.Size([16, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([16, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([16, 9, 50, 50])\n",
      "dec_self_attn_pad_mask shape: torch.Size([16, 50, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([16, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([16, 9, 50, 50])\n",
      "dec_self_attn shape in decoder layer: 16 torch.Size([9, 50, 50])\n",
      "dec_self_attns shape in decoder: 1 torch.Size([16, 9, 50, 50])\n",
      "dec_self_attns shape in transformer: 1 torch.Size([16, 9, 50, 50])\n",
      "val_dec_self_attns shape: 1 torch.Size([16, 9, 50, 50])\n",
      "max mut =  1\n",
      "  peptide\n",
      "0    CFKA\n",
      "1    GFKA\n",
      "2    RFKA\n",
      "3    LFKA\n",
      "4    FCKA\n",
      "5    FRKA\n",
      "6    FKKA\n",
      "7    FLKA\n",
      "8    FFCA\n",
      "9    FFGA\n",
      "# Samples =  16\n",
      "val_loader length: 1\n",
      "val_pep_inputs: torch.Size([16, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([16, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([16, 9, 50, 50])\n",
      "dec_self_attn_pad_mask shape: torch.Size([16, 50, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([16, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([16, 9, 50, 50])\n",
      "dec_self_attn shape in decoder layer: 16 torch.Size([9, 50, 50])\n",
      "dec_self_attns shape in decoder: 1 torch.Size([16, 9, 50, 50])\n",
      "dec_self_attns shape in transformer: 1 torch.Size([16, 9, 50, 50])\n",
      "val_dec_self_attns shape: 1 torch.Size([16, 9, 50, 50])\n",
      "max mut =  1\n",
      "  peptide\n",
      "0    CMRF\n",
      "1    GMRF\n",
      "2    RMRF\n",
      "3    LMRF\n",
      "4    YCRF\n",
      "5    YRRF\n",
      "6    YKRF\n",
      "7    YLRF\n",
      "8    YMCF\n",
      "9    YMGF\n",
      "# Samples =  16\n",
      "val_loader length: 1\n",
      "val_pep_inputs: torch.Size([16, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([16, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([16, 9, 50, 50])\n",
      "dec_self_attn_pad_mask shape: torch.Size([16, 50, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([16, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([16, 9, 50, 50])\n",
      "dec_self_attn shape in decoder layer: 16 torch.Size([9, 50, 50])\n",
      "dec_self_attns shape in decoder: 1 torch.Size([16, 9, 50, 50])\n",
      "dec_self_attns shape in transformer: 1 torch.Size([16, 9, 50, 50])\n",
      "val_dec_self_attns shape: 1 torch.Size([16, 9, 50, 50])\n",
      "max mut =  1\n",
      "  peptide\n",
      "0   CYLPT\n",
      "1   GYLPT\n",
      "2   RYLPT\n",
      "3   LYLPT\n",
      "4   RCLPT\n",
      "5   RRLPT\n",
      "6   RKLPT\n",
      "7   RLLPT\n",
      "8   RYCPT\n",
      "9   RYGPT\n",
      "# Samples =  20\n",
      "val_loader length: 1\n",
      "val_pep_inputs: torch.Size([20, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([20, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([20, 9, 50, 50])\n",
      "dec_self_attn_pad_mask shape: torch.Size([20, 50, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([20, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([20, 9, 50, 50])\n",
      "dec_self_attn shape in decoder layer: 20 torch.Size([9, 50, 50])\n",
      "dec_self_attns shape in decoder: 1 torch.Size([20, 9, 50, 50])\n",
      "dec_self_attns shape in transformer: 1 torch.Size([20, 9, 50, 50])\n",
      "val_dec_self_attns shape: 1 torch.Size([20, 9, 50, 50])\n",
      "max mut =  1\n",
      "  peptide\n",
      "0   CSFGL\n",
      "1   GSFGL\n",
      "2   RSFGL\n",
      "3   LSFGL\n",
      "4   YCFGL\n",
      "5   YRFGL\n",
      "6   YKFGL\n",
      "7   YLFGL\n",
      "8   YSCGL\n",
      "9   YSGGL\n",
      "# Samples =  20\n",
      "val_loader length: 1\n",
      "val_pep_inputs: torch.Size([20, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([20, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([20, 9, 50, 50])\n",
      "dec_self_attn_pad_mask shape: torch.Size([20, 50, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([20, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([20, 9, 50, 50])\n",
      "dec_self_attn shape in decoder layer: 20 torch.Size([9, 50, 50])\n",
      "dec_self_attns shape in decoder: 1 torch.Size([20, 9, 50, 50])\n",
      "dec_self_attns shape in transformer: 1 torch.Size([20, 9, 50, 50])\n",
      "val_dec_self_attns shape: 1 torch.Size([20, 9, 50, 50])\n",
      "max mut =  1\n",
      "  peptide\n",
      "0   CEFFA\n",
      "1   GEFFA\n",
      "2   REFFA\n",
      "3   LEFFA\n",
      "4   ICFFA\n",
      "5   IRFFA\n",
      "6   IKFFA\n",
      "7   ILFFA\n",
      "8   IECFA\n",
      "9   IEGFA\n",
      "# Samples =  20\n",
      "val_loader length: 1\n",
      "val_pep_inputs: torch.Size([20, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([20, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([20, 9, 50, 50])\n",
      "dec_self_attn_pad_mask shape: torch.Size([20, 50, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([20, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([20, 9, 50, 50])\n",
      "dec_self_attn shape in decoder layer: 20 torch.Size([9, 50, 50])\n",
      "dec_self_attns shape in decoder: 1 torch.Size([20, 9, 50, 50])\n",
      "dec_self_attns shape in transformer: 1 torch.Size([20, 9, 50, 50])\n",
      "val_dec_self_attns shape: 1 torch.Size([20, 9, 50, 50])\n",
      "max mut =  1\n",
      "  peptide\n",
      "0   CTTGT\n",
      "1   GTTGT\n",
      "2   RTTGT\n",
      "3   LTTGT\n",
      "4   GCTGT\n",
      "5   GRTGT\n",
      "6   GKTGT\n",
      "7   GLTGT\n",
      "8   GTCGT\n",
      "9   GTGGT\n",
      "# Samples =  20\n",
      "val_loader length: 1\n",
      "val_pep_inputs: torch.Size([20, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([20, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([20, 9, 50, 50])\n",
      "dec_self_attn_pad_mask shape: torch.Size([20, 50, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([20, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([20, 9, 50, 50])\n",
      "dec_self_attn shape in decoder layer: 20 torch.Size([9, 50, 50])\n",
      "dec_self_attns shape in decoder: 1 torch.Size([20, 9, 50, 50])\n",
      "dec_self_attns shape in transformer: 1 torch.Size([20, 9, 50, 50])\n",
      "val_dec_self_attns shape: 1 torch.Size([20, 9, 50, 50])\n",
      "max mut =  1\n",
      "  peptide\n",
      "0   CPPWM\n",
      "1   GPPWM\n",
      "2   RPPWM\n",
      "3   LPPWM\n",
      "4   FCPWM\n",
      "5   FRPWM\n",
      "6   FKPWM\n",
      "7   FLPWM\n",
      "8   FPCWM\n",
      "9   FPGWM\n",
      "# Samples =  20\n",
      "val_loader length: 1\n",
      "val_pep_inputs: torch.Size([20, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([20, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([20, 9, 50, 50])\n",
      "dec_self_attn_pad_mask shape: torch.Size([20, 50, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([20, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([20, 9, 50, 50])\n",
      "dec_self_attn shape in decoder layer: 20 torch.Size([9, 50, 50])\n",
      "dec_self_attns shape in decoder: 1 torch.Size([20, 9, 50, 50])\n",
      "dec_self_attns shape in transformer: 1 torch.Size([20, 9, 50, 50])\n",
      "val_dec_self_attns shape: 1 torch.Size([20, 9, 50, 50])\n",
      "max mut =  1\n",
      "  peptide\n",
      "0   CPPWM\n",
      "1   GPPWM\n",
      "2   RPPWM\n",
      "3   LPPWM\n",
      "4   FCPWM\n",
      "5   FRPWM\n",
      "6   FKPWM\n",
      "7   FLPWM\n",
      "8   FPCWM\n",
      "9   FPGWM\n",
      "# Samples =  20\n",
      "val_loader length: 1\n",
      "val_pep_inputs: torch.Size([20, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([20, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([20, 9, 50, 50])\n",
      "dec_self_attn_pad_mask shape: torch.Size([20, 50, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([20, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([20, 9, 50, 50])\n",
      "dec_self_attn shape in decoder layer: 20 torch.Size([9, 50, 50])\n",
      "dec_self_attns shape in decoder: 1 torch.Size([20, 9, 50, 50])\n",
      "dec_self_attns shape in transformer: 1 torch.Size([20, 9, 50, 50])\n",
      "val_dec_self_attns shape: 1 torch.Size([20, 9, 50, 50])\n",
      "max mut =  1\n",
      "  peptide\n",
      "0   CVHPM\n",
      "1   GVHPM\n",
      "2   RVHPM\n",
      "3   LVHPM\n",
      "4   FCHPM\n",
      "5   FRHPM\n",
      "6   FKHPM\n",
      "7   FLHPM\n",
      "8   FVCPM\n",
      "9   FVGPM\n",
      "# Samples =  20\n",
      "val_loader length: 1\n",
      "val_pep_inputs: torch.Size([20, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([20, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([20, 9, 50, 50])\n",
      "dec_self_attn_pad_mask shape: torch.Size([20, 50, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([20, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([20, 9, 50, 50])\n",
      "dec_self_attn shape in decoder layer: 20 torch.Size([9, 50, 50])\n",
      "dec_self_attns shape in decoder: 1 torch.Size([20, 9, 50, 50])\n",
      "dec_self_attns shape in transformer: 1 torch.Size([20, 9, 50, 50])\n",
      "val_dec_self_attns shape: 1 torch.Size([20, 9, 50, 50])\n",
      "max mut =  1\n",
      "  peptide\n",
      "0   CVHPM\n",
      "1   GVHPM\n",
      "2   RVHPM\n",
      "3   LVHPM\n",
      "4   FCHPM\n",
      "5   FRHPM\n",
      "6   FKHPM\n",
      "7   FLHPM\n",
      "8   FVCPM\n",
      "9   FVGPM\n",
      "# Samples =  20\n",
      "val_loader length: 1\n",
      "val_pep_inputs: torch.Size([20, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([20, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([20, 9, 50, 50])\n",
      "dec_self_attn_pad_mask shape: torch.Size([20, 50, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([20, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([20, 9, 50, 50])\n",
      "dec_self_attn shape in decoder layer: 20 torch.Size([9, 50, 50])\n",
      "dec_self_attns shape in decoder: 1 torch.Size([20, 9, 50, 50])\n",
      "dec_self_attns shape in transformer: 1 torch.Size([20, 9, 50, 50])\n",
      "val_dec_self_attns shape: 1 torch.Size([20, 9, 50, 50])\n",
      "max mut =  1\n",
      "  peptide\n",
      "0   CPPLC\n",
      "1   GPPLC\n",
      "2   RPPLC\n",
      "3   LPPLC\n",
      "4   CCPLC\n",
      "5   CRPLC\n",
      "6   CKPLC\n",
      "7   CLPLC\n",
      "8   CPCLC\n",
      "9   CPGLC\n",
      "# Samples =  20\n",
      "val_loader length: 1\n",
      "val_pep_inputs: torch.Size([20, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([20, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([20, 9, 50, 50])\n",
      "dec_self_attn_pad_mask shape: torch.Size([20, 50, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([20, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([20, 9, 50, 50])\n",
      "dec_self_attn shape in decoder layer: 20 torch.Size([9, 50, 50])\n",
      "dec_self_attns shape in decoder: 1 torch.Size([20, 9, 50, 50])\n",
      "dec_self_attns shape in transformer: 1 torch.Size([20, 9, 50, 50])\n",
      "val_dec_self_attns shape: 1 torch.Size([20, 9, 50, 50])\n",
      "max mut =  1\n",
      "  peptide\n",
      "0   CPLRF\n",
      "1   GPLRF\n",
      "2   RPLRF\n",
      "3   LPLRF\n",
      "4   LCLRF\n",
      "5   LRLRF\n",
      "6   LKLRF\n",
      "7   LLLRF\n",
      "8   LPCRF\n",
      "9   LPGRF\n",
      "# Samples =  20\n",
      "val_loader length: 1\n",
      "val_pep_inputs: torch.Size([20, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([20, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([20, 9, 50, 50])\n",
      "dec_self_attn_pad_mask shape: torch.Size([20, 50, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([20, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([20, 9, 50, 50])\n",
      "dec_self_attn shape in decoder layer: 20 torch.Size([9, 50, 50])\n",
      "dec_self_attns shape in decoder: 1 torch.Size([20, 9, 50, 50])\n",
      "dec_self_attns shape in transformer: 1 torch.Size([20, 9, 50, 50])\n",
      "val_dec_self_attns shape: 1 torch.Size([20, 9, 50, 50])\n",
      "max mut =  1\n",
      "  peptide\n",
      "0   CDFFA\n",
      "1   GDFFA\n",
      "2   RDFFA\n",
      "3   LDFFA\n",
      "4   VCFFA\n",
      "5   VRFFA\n",
      "6   VKFFA\n",
      "7   VLFFA\n",
      "8   VDCFA\n",
      "9   VDGFA\n",
      "# Samples =  20\n",
      "val_loader length: 1\n",
      "val_pep_inputs: torch.Size([20, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([20, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([20, 9, 50, 50])\n",
      "dec_self_attn_pad_mask shape: torch.Size([20, 50, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([20, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([20, 9, 50, 50])\n",
      "dec_self_attn shape in decoder layer: 20 torch.Size([9, 50, 50])\n",
      "dec_self_attns shape in decoder: 1 torch.Size([20, 9, 50, 50])\n",
      "dec_self_attns shape in transformer: 1 torch.Size([20, 9, 50, 50])\n",
      "val_dec_self_attns shape: 1 torch.Size([20, 9, 50, 50])\n",
      "max mut =  1\n",
      "  peptide\n",
      "0   CAAPF\n",
      "1   GAAPF\n",
      "2   RAAPF\n",
      "3   LAAPF\n",
      "4   ACAPF\n",
      "5   ARAPF\n",
      "6   AKAPF\n",
      "7   ALAPF\n",
      "8   AACPF\n",
      "9   AAGPF\n",
      "# Samples =  20\n",
      "val_loader length: 1\n",
      "val_pep_inputs: torch.Size([20, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([20, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([20, 9, 50, 50])\n",
      "dec_self_attn_pad_mask shape: torch.Size([20, 50, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([20, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([20, 9, 50, 50])\n",
      "dec_self_attn shape in decoder layer: 20 torch.Size([9, 50, 50])\n",
      "dec_self_attns shape in decoder: 1 torch.Size([20, 9, 50, 50])\n",
      "dec_self_attns shape in transformer: 1 torch.Size([20, 9, 50, 50])\n",
      "val_dec_self_attns shape: 1 torch.Size([20, 9, 50, 50])\n",
      "max mut =  1\n",
      "  peptide\n",
      "0   CGFFT\n",
      "1   GGFFT\n",
      "2   RGFFT\n",
      "3   LGFFT\n",
      "4   VCFFT\n",
      "5   VRFFT\n",
      "6   VKFFT\n",
      "7   VLFFT\n",
      "8   VGCFT\n",
      "9   VGGFT\n",
      "# Samples =  20\n",
      "val_loader length: 1\n",
      "val_pep_inputs: torch.Size([20, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([20, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([20, 9, 50, 50])\n",
      "dec_self_attn_pad_mask shape: torch.Size([20, 50, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([20, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([20, 9, 50, 50])\n",
      "dec_self_attn shape in decoder layer: 20 torch.Size([9, 50, 50])\n",
      "dec_self_attns shape in decoder: 1 torch.Size([20, 9, 50, 50])\n",
      "dec_self_attns shape in transformer: 1 torch.Size([20, 9, 50, 50])\n",
      "val_dec_self_attns shape: 1 torch.Size([20, 9, 50, 50])\n",
      "max mut =  1\n",
      "  peptide\n",
      "0   CFSPQ\n",
      "1   GFSPQ\n",
      "2   RFSPQ\n",
      "3   LFSPQ\n",
      "4   MCSPQ\n",
      "5   MRSPQ\n",
      "6   MKSPQ\n",
      "7   MLSPQ\n",
      "8   MFCPQ\n",
      "9   MFGPQ\n",
      "# Samples =  20\n",
      "val_loader length: 1\n",
      "val_pep_inputs: torch.Size([20, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([20, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([20, 9, 50, 50])\n",
      "dec_self_attn_pad_mask shape: torch.Size([20, 50, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([20, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([20, 9, 50, 50])\n",
      "dec_self_attn shape in decoder layer: 20 torch.Size([9, 50, 50])\n",
      "dec_self_attns shape in decoder: 1 torch.Size([20, 9, 50, 50])\n",
      "dec_self_attns shape in transformer: 1 torch.Size([20, 9, 50, 50])\n",
      "val_dec_self_attns shape: 1 torch.Size([20, 9, 50, 50])\n",
      "max mut =  1\n",
      "  peptide\n",
      "0   CEFFT\n",
      "1   GEFFT\n",
      "2   REFFT\n",
      "3   LEFFT\n",
      "4   ICFFT\n",
      "5   IRFFT\n",
      "6   IKFFT\n",
      "7   ILFFT\n",
      "8   IECFT\n",
      "9   IEGFT\n",
      "# Samples =  20\n",
      "val_loader length: 1\n",
      "val_pep_inputs: torch.Size([20, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([20, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([20, 9, 50, 50])\n",
      "dec_self_attn_pad_mask shape: torch.Size([20, 50, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([20, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([20, 9, 50, 50])\n",
      "dec_self_attn shape in decoder layer: 20 torch.Size([9, 50, 50])\n",
      "dec_self_attns shape in decoder: 1 torch.Size([20, 9, 50, 50])\n",
      "dec_self_attns shape in transformer: 1 torch.Size([20, 9, 50, 50])\n",
      "val_dec_self_attns shape: 1 torch.Size([20, 9, 50, 50])\n",
      "max mut =  2\n",
      "  peptide\n",
      "0  CPTNLH\n",
      "1  GPTNLH\n",
      "2  RPTNLH\n",
      "3  LPTNLH\n",
      "4  NCTNLH\n",
      "5  NRTNLH\n",
      "6  NKTNLH\n",
      "7  NLTNLH\n",
      "8  NPCNLH\n",
      "9  NPGNLH\n",
      "# Samples =  264\n",
      "val_loader length: 1\n",
      "val_pep_inputs: torch.Size([264, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([264, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([264, 9, 50, 50])\n",
      "dec_self_attn_pad_mask shape: torch.Size([264, 50, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([264, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([264, 9, 50, 50])\n",
      "dec_self_attn shape in decoder layer: 264 torch.Size([9, 50, 50])\n",
      "dec_self_attns shape in decoder: 1 torch.Size([264, 9, 50, 50])\n",
      "dec_self_attns shape in transformer: 1 torch.Size([264, 9, 50, 50])\n",
      "val_dec_self_attns shape: 1 torch.Size([264, 9, 50, 50])\n",
      "max mut =  2\n",
      "  peptide\n",
      "0  CPWDWV\n",
      "1  GPWDWV\n",
      "2  RPWDWV\n",
      "3  LPWDWV\n",
      "4  DCWDWV\n",
      "5  DRWDWV\n",
      "6  DKWDWV\n",
      "7  DLWDWV\n",
      "8  DPCDWV\n",
      "9  DPGDWV\n",
      "# Samples =  264\n",
      "val_loader length: 1\n",
      "val_pep_inputs: torch.Size([264, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([264, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([264, 9, 50, 50])\n",
      "dec_self_attn_pad_mask shape: torch.Size([264, 50, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([264, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([264, 9, 50, 50])\n",
      "dec_self_attn shape in decoder layer: 264 torch.Size([9, 50, 50])\n",
      "dec_self_attns shape in decoder: 1 torch.Size([264, 9, 50, 50])\n",
      "dec_self_attns shape in transformer: 1 torch.Size([264, 9, 50, 50])\n",
      "val_dec_self_attns shape: 1 torch.Size([264, 9, 50, 50])\n",
      "max mut =  2\n",
      "  peptide\n",
      "0  CPWDWV\n",
      "1  GPWDWV\n",
      "2  RPWDWV\n",
      "3  LPWDWV\n",
      "4  DCWDWV\n",
      "5  DRWDWV\n",
      "6  DKWDWV\n",
      "7  DLWDWV\n",
      "8  DPCDWV\n",
      "9  DPGDWV\n",
      "# Samples =  264\n",
      "val_loader length: 1\n",
      "val_pep_inputs: torch.Size([264, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([264, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([264, 9, 50, 50])\n",
      "dec_self_attn_pad_mask shape: torch.Size([264, 50, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([264, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([264, 9, 50, 50])\n",
      "dec_self_attn shape in decoder layer: 264 torch.Size([9, 50, 50])\n",
      "dec_self_attns shape in decoder: 1 torch.Size([264, 9, 50, 50])\n",
      "dec_self_attns shape in transformer: 1 torch.Size([264, 9, 50, 50])\n",
      "val_dec_self_attns shape: 1 torch.Size([264, 9, 50, 50])\n",
      "max mut =  2\n",
      "  peptide\n",
      "0  CFFAGP\n",
      "1  GFFAGP\n",
      "2  RFFAGP\n",
      "3  LFFAGP\n",
      "4  VCFAGP\n",
      "5  VRFAGP\n",
      "6  VKFAGP\n",
      "7  VLFAGP\n",
      "8  VFCAGP\n",
      "9  VFGAGP\n",
      "# Samples =  264\n",
      "val_loader length: 1\n",
      "val_pep_inputs: torch.Size([264, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([264, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([264, 9, 50, 50])\n",
      "dec_self_attn_pad_mask shape: torch.Size([264, 50, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([264, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([264, 9, 50, 50])\n",
      "dec_self_attn shape in decoder layer: 264 torch.Size([9, 50, 50])\n",
      "dec_self_attns shape in decoder: 1 torch.Size([264, 9, 50, 50])\n",
      "dec_self_attns shape in transformer: 1 torch.Size([264, 9, 50, 50])\n",
      "val_dec_self_attns shape: 1 torch.Size([264, 9, 50, 50])\n",
      "max mut =  2\n",
      "  peptide\n",
      "0  CIDPGV\n",
      "1  GIDPGV\n",
      "2  RIDPGV\n",
      "3  LIDPGV\n",
      "4  PCDPGV\n",
      "5  PRDPGV\n",
      "6  PKDPGV\n",
      "7  PLDPGV\n",
      "8  PICPGV\n",
      "9  PIGPGV\n",
      "# Samples =  264\n",
      "val_loader length: 1\n",
      "val_pep_inputs: torch.Size([264, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([264, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([264, 9, 50, 50])\n",
      "dec_self_attn_pad_mask shape: torch.Size([264, 50, 50])\n",
      "attn shape in scaled dot product attention: torch.Size([264, 9, 50, 50])\n",
      "attn shape in multi head attention: torch.Size([264, 9, 50, 50])\n",
      "dec_self_attn shape in decoder layer: 264 torch.Size([9, 50, 50])\n",
      "dec_self_attns shape in decoder: 1 torch.Size([264, 9, 50, 50])\n",
      "dec_self_attns shape in transformer: 1 torch.Size([264, 9, 50, 50])\n",
      "val_dec_self_attns shape: 1 torch.Size([264, 9, 50, 50])\n",
      "max mut =  2\n",
      "  peptide\n",
      "0  CNFFRF\n",
      "1  GNFFRF\n",
      "2  RNFFRF\n",
      "3  LNFFRF\n",
      "4  GCFFRF\n",
      "5  GRFFRF\n",
      "6  GKFFRF\n",
      "7  GLFFRF\n",
      "8  GNCFRF\n",
      "9  GNGFRF\n",
      "# Samples =  264\n",
      "val_loader length: 1\n",
      "val_pep_inputs: torch.Size([264, 50])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 100\u001b[0m\n\u001b[0;32m     98\u001b[0m predict_data, predict_pep_inputs, predict_loader \u001b[38;5;241m=\u001b[39m read_predict_data(mutated_df, batch_size)\n\u001b[0;32m     99\u001b[0m model_eval\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m--> 100\u001b[0m y_pred, y_prob, attns \u001b[38;5;241m=\u001b[39m \u001b[43meval_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cuda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m predict_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m'\u001b[39m], predict_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_prob\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m y_pred, y_prob\n\u001b[0;32m    103\u001b[0m predict_data \u001b[38;5;241m=\u001b[39m predict_data\u001b[38;5;241m.\u001b[39mround({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_prob\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m4\u001b[39m})\n",
      "File \u001b[1;32md:\\Faks\\4. godina\\Evolucijsko računarstvo\\Projekt\\antimicrobial\\model.py:377\u001b[0m, in \u001b[0;36meval_step\u001b[1;34m(model, val_loader, threshold, use_cuda)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_pep_inputs:\u001b[39m\u001b[38;5;124m\"\u001b[39m, val_pep_inputs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    376\u001b[0m val_pep_inputs \u001b[38;5;241m=\u001b[39m val_pep_inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 377\u001b[0m val_outputs, _, val_dec_self_attns \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_pep_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    379\u001b[0m y_prob_val \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSoftmax(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)(val_outputs)[:, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    380\u001b[0m y_prob_val_list\u001b[38;5;241m.\u001b[39mextend(y_prob_val)\n",
      "File \u001b[1;32mc:\\Users\\LukaBursic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LukaBursic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Faks\\4. godina\\Evolucijsko računarstvo\\Projekt\\antimicrobial\\model.py:352\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, pep_inputs)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03mpep_inputs: [batch_size, pep_len]\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03mhla_inputs: [batch_size, hla_len]\u001b[39;00m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;66;03m# tensor to store decoder outputs\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;66;03m# outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# enc_outputs: [batch_size, src_len, d_model], enc_self_attns: [n_layers, batch_size, n_heads, src_len, src_len]\u001b[39;00m\n\u001b[1;32m--> 352\u001b[0m pep_enc_outputs, pep_enc_self_attns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpep_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpep_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# hla_enc_outputs, hla_enc_self_attns = self.hla_encoder(hla_inputs)\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;66;03m# enc_outputs = torch.cat((pep_enc_outputs, hla_enc_outputs), 1) # concat pep & hla embedding\u001b[39;00m\n\u001b[0;32m    355\u001b[0m enc_outputs \u001b[38;5;241m=\u001b[39m pep_enc_outputs\n",
      "File \u001b[1;32mc:\\Users\\LukaBursic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LukaBursic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Faks\\4. godina\\Evolucijsko računarstvo\\Projekt\\antimicrobial\\model.py:266\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, enc_inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m enc_self_attns \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;66;03m# enc_outputs: [batch_size, src_len, d_model], enc_self_attn: [batch_size, n_heads, src_len, src_len]\u001b[39;00m\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;66;03m# output n-tog layera je input n+1-og layera (zato layeru kao input dajemo enc_output) \u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m     enc_outputs, enc_self_attn \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_self_attn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m     enc_self_attns\u001b[38;5;241m.\u001b[39mappend(enc_self_attn)\n\u001b[0;32m    268\u001b[0m \u001b[38;5;66;03m# vraca se output samo od zadnjeg layera, i attention od svih layera\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LukaBursic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LukaBursic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Faks\\4. godina\\Evolucijsko računarstvo\\Projekt\\antimicrobial\\model.py:244\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[1;34m(self, enc_inputs, enc_self_attn_mask)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;124;03menc_inputs: [batch_size, src_len, d_model]\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;124;03menc_self_attn_mask: [batch_size, src_len, src_len]\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;66;03m# enc_outputs: [batch_size, src_len, d_model], attn: [batch_size, n_heads, src_len, src_len]\u001b[39;00m\n\u001b[1;32m--> 244\u001b[0m enc_outputs, attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menc_self_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_self_attn_mask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# enc_inputs to same Q,K,V\u001b[39;00m\n\u001b[0;32m    245\u001b[0m enc_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_ffn(enc_outputs) \u001b[38;5;66;03m# enc_outputs: [batch_size, src_len, d_model]\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m enc_outputs, attn\n",
      "File \u001b[1;32mc:\\Users\\LukaBursic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LukaBursic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Faks\\4. godina\\Evolucijsko računarstvo\\Projekt\\antimicrobial\\model.py:205\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, input_Q, input_K, input_V, attn_mask)\u001b[0m\n\u001b[0;32m    202\u001b[0m attn_mask \u001b[38;5;241m=\u001b[39m attn_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, n_heads, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# attn_mask : [batch_size, n_heads, seq_len, seq_len]\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;66;03m# context: [batch_size, n_heads, len_q, d_v], attn: [batch_size, n_heads, len_q, len_k]\u001b[39;00m\n\u001b[1;32m--> 205\u001b[0m context, attn \u001b[38;5;241m=\u001b[39m \u001b[43mScaledDotProductAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m context \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, n_heads \u001b[38;5;241m*\u001b[39m d_v) \u001b[38;5;66;03m# context: [batch_size, len_q, n_heads * d_v]\u001b[39;00m\n\u001b[0;32m    207\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(context) \u001b[38;5;66;03m# [batch_size, len_q, d_model]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LukaBursic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LukaBursic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Faks\\4. godina\\Evolucijsko računarstvo\\Projekt\\antimicrobial\\model.py:172\u001b[0m, in \u001b[0;36mScaledDotProductAttention.forward\u001b[1;34m(self, Q, K, V, attn_mask)\u001b[0m\n\u001b[0;32m    169\u001b[0m scores\u001b[38;5;241m.\u001b[39mmasked_fill_(attn_mask, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1e9\u001b[39m) \u001b[38;5;66;03m# Fills elements of self tensor with value where mask is True.\u001b[39;00m\n\u001b[0;32m    171\u001b[0m attn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)(scores)\n\u001b[1;32m--> 172\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# [batch_size, n_heads, len_q, d_v]\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn shape in scaled dot product attention:\u001b[39m\u001b[38;5;124m\"\u001b[39m, attn\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m context, attn\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# # Define the mutate_sequence function\n",
    "# def mutate_sequence(sequence):\n",
    "#     ls = []\n",
    "#     ls.append(sequence.replace('G', 'C'))\n",
    "#     ls.append(sequence.replace('G', 'L'))\n",
    "#     ls.append(sequence.replace('G', 'K'))\n",
    "#     return ls\n",
    "\n",
    "# def mutate_sequence(sequence):\n",
    "#     print('----------')\n",
    "#     top_aa = pd.read_csv('top_amino_acids.csv')\n",
    "#     # print(top_aa['Top 1'][0])\n",
    "#     ls = []\n",
    "    \n",
    "#     for i in range(len(sequence)):\n",
    "#         seq_list = list(sequence)  # Convert sequence to a list of characters\n",
    "#         seq_list[i] = top_aa['Top 1'][i]\n",
    "#         mutated_sequence = ''.join(seq_list)  # Join the list back into a string\n",
    "#         ls.append(mutated_sequence)\n",
    "\n",
    "#         seq_list = list(sequence)  # Convert sequence to a list of characters\n",
    "#         seq_list[i] = top_aa['Top 2'][i]\n",
    "#         mutated_sequence = ''.join(seq_list)  # Join the list back into a string\n",
    "#         ls.append(mutated_sequence)\n",
    "\n",
    "#         seq_list = list(sequence)  # Convert sequence to a list of characters\n",
    "#         seq_list[i] = top_aa['Top 3'][i]\n",
    "#         mutated_sequence = ''.join(seq_list)  # Join the list back into a string\n",
    "#         ls.append(mutated_sequence)\n",
    "\n",
    "#         seq_list = list(sequence)  # Convert sequence to a list of characters\n",
    "#         seq_list[i] = top_aa['Top 4'][i]\n",
    "#         mutated_sequence = ''.join(seq_list)  # Join the list back into a string\n",
    "#         ls.append(mutated_sequence)\n",
    "        \n",
    "#     return ls\n",
    "\n",
    "import pandas as pd\n",
    "from itertools import combinations, product\n",
    "\n",
    "def mutate_sequence(sequence, max_mutations=4):\n",
    "    top_aa = pd.read_csv('top_amino_acids.csv')\n",
    "    ls = []\n",
    "    \n",
    "    # Function to apply mutations to a sequence\n",
    "    def apply_mutations(seq, mutations):\n",
    "        seq_list = list(seq)\n",
    "        for pos, top_idx in mutations:\n",
    "            seq_list[pos] = top_aa.iloc[pos, top_idx]\n",
    "        return ''.join(seq_list)\n",
    "\n",
    "    print(\"max mut = \", max_mutations)\n",
    "    # Generate mutations for 1 to max_mutations\n",
    "    for num_mutations in range(1, max_mutations + 1):\n",
    "        for positions in combinations(range(len(sequence)), num_mutations):\n",
    "            # For each combination of positions, generate the possible mutations\n",
    "            mutation_indices = [range(1, 5)] * num_mutations\n",
    "            for mutation_combination in product(*mutation_indices):\n",
    "                mutations = list(zip(positions, mutation_combination))\n",
    "                mutated_sequence = apply_mutations(sequence, mutations)\n",
    "                ls.append(mutated_sequence)\n",
    "    \n",
    "    return ls\n",
    "\n",
    "# Example usage\n",
    "# true_negative_samples = pd.read_csv(\"true_negative_samples.csv\")\n",
    "# print(mutate_sequence(true_negative_samples['sequence'][0]))\n",
    "\n",
    "\n",
    "# Read the original dataset\n",
    "data = pd.read_csv('./true_negative_samples.csv')\n",
    "\n",
    "# Create a list to store the new sequences and their labels\n",
    "\n",
    "\n",
    "# Iterate over the original sequences and generate mutations\n",
    "for i in range(len(data)):\n",
    "    mutated_data = []\n",
    "    original_sequence = data['sequence'][i]\n",
    "    original_label = data['label'][i]\n",
    "    mutations = mutate_sequence(original_sequence, max_mutations=math.ceil(0.2*len(original_sequence)))\n",
    "    \n",
    "    # Add the original sequence to the new dataset\n",
    "    # mutated_data.append({'sequence': original_sequence, 'label': original_label})\n",
    "    \n",
    "    # Add the mutated sequences to the new dataset\n",
    "    for mutated_sequence in mutations:\n",
    "        mutated_data.append({'peptide': mutated_sequence})\n",
    "\n",
    "    # Create a new DataFrame from the mutated_data list\n",
    "    mutated_df = pd.DataFrame(mutated_data)\n",
    "\n",
    "    # Display the new DataFrame\n",
    "    print(mutated_df.head(10))\n",
    "\n",
    "    predict_data, predict_pep_inputs, predict_loader = read_predict_data(mutated_df, batch_size)\n",
    "    model_eval.eval()\n",
    "    y_pred, y_prob, attns = eval_step(model_eval, predict_loader, 0.5, use_cuda)\n",
    "\n",
    "    predict_data['y_pred'], predict_data['y_prob'] = y_pred, y_prob\n",
    "    predict_data = predict_data.round({'y_prob': 4})\n",
    "\n",
    "    predict_data.to_csv(f'mutated_datasets_new/mutated_dataset_{original_sequence}_new.csv', index = False)\n",
    "\n",
    "    # Optionally, save the new DataFrame to a CSV file\n",
    "    # mutated_df.to_csv(f'mutated_datasets/mutated_dataset_{original_sequence}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('CEP', 1, 0.6666666666666667), ('GEP', 1, 0.6666666666666667), ('REP', 1, 0.6666666666666667), ('LEP', 1, 0.6666666666666667), ('GCP', 1, 0.6666666666666667), ('GRP', 1, 0.6666666666666667), ('GKP', 1, 0.6666666666666667), ('GLP', 1, 0.6666666666666667), ('GEC', 1, 0.6666666666666667), ('GEG', 1, 0.6666666666666667), ('GEL', 1, 0.6666666666666667), ('GEV', 1, 0.6666666666666667), ('CCP', 2, 0.33333333333333337), ('CRP', 2, 0.33333333333333337), ('CKP', 2, 0.33333333333333337), ('CLP', 2, 0.33333333333333337), ('GCP', 2, 0.33333333333333337), ('GRP', 2, 0.33333333333333337), ('GKP', 2, 0.33333333333333337), ('GLP', 2, 0.33333333333333337), ('RCP', 2, 0.33333333333333337), ('RRP', 2, 0.33333333333333337), ('RKP', 2, 0.33333333333333337), ('RLP', 2, 0.33333333333333337), ('LCP', 2, 0.33333333333333337), ('LRP', 2, 0.33333333333333337), ('LKP', 2, 0.33333333333333337), ('LLP', 2, 0.33333333333333337), ('CEC', 2, 0.33333333333333337), ('CEG', 2, 0.33333333333333337), ('CEL', 2, 0.33333333333333337), ('CEV', 2, 0.33333333333333337), ('GEC', 2, 0.33333333333333337), ('GEG', 2, 0.33333333333333337), ('GEL', 2, 0.33333333333333337), ('GEV', 2, 0.33333333333333337), ('REC', 2, 0.33333333333333337), ('REG', 2, 0.33333333333333337), ('REL', 2, 0.33333333333333337), ('REV', 2, 0.33333333333333337), ('LEC', 2, 0.33333333333333337), ('LEG', 2, 0.33333333333333337), ('LEL', 2, 0.33333333333333337), ('LEV', 2, 0.33333333333333337), ('GCC', 2, 0.33333333333333337), ('GCG', 2, 0.33333333333333337), ('GCL', 2, 0.33333333333333337), ('GCV', 2, 0.33333333333333337), ('GRC', 2, 0.33333333333333337), ('GRG', 2, 0.33333333333333337), ('GRL', 2, 0.33333333333333337), ('GRV', 2, 0.33333333333333337), ('GKC', 2, 0.33333333333333337), ('GKG', 2, 0.33333333333333337), ('GKL', 2, 0.33333333333333337), ('GKV', 2, 0.33333333333333337), ('GLC', 2, 0.33333333333333337), ('GLG', 2, 0.33333333333333337), ('GLL', 2, 0.33333333333333337), ('GLV', 2, 0.33333333333333337), ('CCC', 3, 0.0), ('CCG', 3, 0.0), ('CCL', 3, 0.0), ('CCV', 3, 0.0), ('CRC', 3, 0.0), ('CRG', 3, 0.0), ('CRL', 3, 0.0), ('CRV', 3, 0.0), ('CKC', 3, 0.0), ('CKG', 3, 0.0), ('CKL', 3, 0.0), ('CKV', 3, 0.0), ('CLC', 3, 0.0), ('CLG', 3, 0.0), ('CLL', 3, 0.0), ('CLV', 3, 0.0), ('GCC', 3, 0.0), ('GCG', 3, 0.0), ('GCL', 3, 0.0), ('GCV', 3, 0.0), ('GRC', 3, 0.0), ('GRG', 3, 0.0), ('GRL', 3, 0.0), ('GRV', 3, 0.0), ('GKC', 3, 0.0), ('GKG', 3, 0.0), ('GKL', 3, 0.0), ('GKV', 3, 0.0), ('GLC', 3, 0.0), ('GLG', 3, 0.0), ('GLL', 3, 0.0), ('GLV', 3, 0.0), ('RCC', 3, 0.0), ('RCG', 3, 0.0), ('RCL', 3, 0.0), ('RCV', 3, 0.0), ('RRC', 3, 0.0), ('RRG', 3, 0.0), ('RRL', 3, 0.0), ('RRV', 3, 0.0), ('RKC', 3, 0.0), ('RKG', 3, 0.0), ('RKL', 3, 0.0), ('RKV', 3, 0.0), ('RLC', 3, 0.0), ('RLG', 3, 0.0), ('RLL', 3, 0.0), ('RLV', 3, 0.0), ('LCC', 3, 0.0), ('LCG', 3, 0.0), ('LCL', 3, 0.0), ('LCV', 3, 0.0), ('LRC', 3, 0.0), ('LRG', 3, 0.0), ('LRL', 3, 0.0), ('LRV', 3, 0.0), ('LKC', 3, 0.0), ('LKG', 3, 0.0), ('LKL', 3, 0.0), ('LKV', 3, 0.0), ('LLC', 3, 0.0), ('LLG', 3, 0.0), ('LLL', 3, 0.0), ('LLV', 3, 0.0)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mutated Sequence</th>\n",
       "      <th>Number of Mutations</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CEP</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GEP</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>REP</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LEP</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GCP</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>LKV</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>LLC</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>LLG</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>LLL</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>LLV</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Mutated Sequence  Number of Mutations  Similarity\n",
       "0                CEP                    1    0.666667\n",
       "1                GEP                    1    0.666667\n",
       "2                REP                    1    0.666667\n",
       "3                LEP                    1    0.666667\n",
       "4                GCP                    1    0.666667\n",
       "..               ...                  ...         ...\n",
       "119              LKV                    3    0.000000\n",
       "120              LLC                    3    0.000000\n",
       "121              LLG                    3    0.000000\n",
       "122              LLL                    3    0.000000\n",
       "123              LLV                    3    0.000000\n",
       "\n",
       "[124 rows x 3 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations, product\n",
    "\n",
    "def mutate_sequence(sequence, max_mutations=4):\n",
    "    top_aa = pd.read_csv('top_amino_acids.csv')\n",
    "    mutated_sequences = []\n",
    "    \n",
    "    # Function to apply mutations to a sequence\n",
    "    def apply_mutations(seq, mutations):\n",
    "        seq_list = list(seq)\n",
    "        for pos, top_idx in mutations:\n",
    "            seq_list[pos] = top_aa.iloc[pos, top_idx]\n",
    "        return ''.join(seq_list)\n",
    "\n",
    "    # Generate mutations for 1 to max_mutations\n",
    "    for num_mutations in range(1, max_mutations + 1):\n",
    "        similarity = 1 - num_mutations/len(sequence)\n",
    "        for positions in combinations(range(len(sequence)), num_mutations):\n",
    "            # For each combination of positions, generate the possible mutations\n",
    "            mutation_indices = [range(1, 5)] * num_mutations\n",
    "            for mutation_combination in product(*mutation_indices):\n",
    "                mutations = list(zip(positions, mutation_combination))\n",
    "                mutated_sequence = apply_mutations(sequence, mutations)\n",
    "                mutated_sequences.append((mutated_sequence, num_mutations, similarity))\n",
    "    \n",
    "    return mutated_sequences\n",
    "\n",
    "# Example usage\n",
    "true_negative_samples = pd.read_csv(\"true_negative_samples.csv\")\n",
    "mutated_sequences_with_mutations = mutate_sequence(true_negative_samples['sequence'][0], max_mutations=ceil(0.2*len(true_negative_samples['sequence'][0])))\n",
    "print(mutated_sequences_with_mutations)\n",
    "\n",
    "df = pd.DataFrame(mutated_sequences_with_mutations, columns=['Mutated Sequence', 'Number of Mutations', 'Similarity'])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mut_peptides_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mut_data, _, _, mut_loader \u001b[38;5;241m=\u001b[39m read_predict_data(\u001b[43mmut_peptides_df\u001b[49m, batch_size)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mut_peptides_df' is not defined"
     ]
    }
   ],
   "source": [
    "mut_data, _, _, mut_loader = read_predict_data(mut_peptides_df, batch_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
